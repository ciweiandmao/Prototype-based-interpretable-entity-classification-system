import torch
import torch.nn as nn
import torch.nn.functional as F
import dgl
import dgl.nn as dglnn
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler, LabelEncoder
from collections import defaultdict, Counter
import os
from tqdm import tqdm
import warnings

warnings.filterwarnings('ignore')


class MultiModalGraphSAGE(nn.Module):
    """å¤šæ¨¡æ€GraphSAGEæ¨¡å‹ï¼ˆä¸è®­ç»ƒä»£ç å®Œå…¨ä¸€è‡´ï¼‰"""

    def __init__(self, structural_dim, multimodal_dim, h_feats, num_classes, num_layers=2, dropout=0.3):
        super().__init__()

        total_in_feats = structural_dim + multimodal_dim

        # è¾“å…¥ç¼–ç å±‚
        self.input_encoder = nn.Sequential(
            nn.Linear(total_in_feats, h_feats),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        self.num_layers = num_layers
        self.dropout = dropout

        # GraphSAGEå±‚
        self.sage_layers = nn.ModuleList()
        self.bns = nn.ModuleList()

        # ç¬¬1å±‚
        self.sage_layers.append(dglnn.SAGEConv(
            in_feats=h_feats,
            out_feats=h_feats * 2,
            aggregator_type='mean',
            feat_drop=dropout
        ))
        self.bns.append(nn.BatchNorm1d(h_feats * 2))

        # ä¸­é—´å±‚
        for i in range(1, num_layers - 1):
            self.sage_layers.append(dglnn.SAGEConv(
                in_feats=h_feats * 2,
                out_feats=h_feats * 2,
                aggregator_type='mean',
                feat_drop=dropout
            ))
            self.bns.append(nn.BatchNorm1d(h_feats * 2))

        # è¾“å‡ºå±‚
        if num_layers > 1:
            self.sage_layers.append(dglnn.SAGEConv(
                in_feats=h_feats * 2,
                out_feats=h_feats,
                aggregator_type='mean',
                feat_drop=dropout
            ))
            self.bns.append(nn.BatchNorm1d(h_feats))

        # å…³ç³»ç±»å‹ç¼–ç å™¨
        self.relation_encoder = nn.Sequential(
            nn.Linear(h_feats * 2, h_feats),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        # ç±»å‹æ¨¡å¼èšåˆå±‚
        self.type_pattern_aggregator = nn.Sequential(
            nn.Linear(h_feats * 3, h_feats * 2),
            nn.ReLU(),
            nn.Dropout(dropout)
        )

        # è¾“å‡ºåˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(h_feats, h_feats // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(h_feats // 2, num_classes)
        )

    def forward(self, g, structural_features, multimodal_features):
        # èåˆç‰¹å¾
        combined_features = torch.cat([structural_features, multimodal_features], dim=-1)

        # è¾“å…¥ç¼–ç 
        h = self.input_encoder(combined_features)

        # GraphSAGEä¼ æ’­
        layer_outputs = [h]
        for i in range(self.num_layers):
            h = self.sage_layers[i](g, h)
            h = self.bns[i](h)
            h = F.relu(h)
            h = F.dropout(h, p=self.dropout, training=self.training)
            layer_outputs.append(h)

        # å¤šå±‚ç‰¹å¾èåˆ
        if len(layer_outputs) > 1:
            h_final = torch.cat([layer_outputs[0], layer_outputs[-1]], dim=1)
            h_final = self.relation_encoder(h_final)
        else:
            h_final = layer_outputs[0]

        # æœ€ç»ˆåˆ†ç±»
        out = self.classifier(h_final)

        return out


class MultiModalEncoder:
    """å¤šæ¨¡æ€ç¼–ç å™¨ï¼ˆä¸è®­ç»ƒä»£ç ä¸€è‡´ï¼‰"""

    def __init__(self):
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # æ•°å€¼ç‰¹å¾ç¼–ç å™¨ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
        self.numeric_encoder = nn.Sequential(
            nn.Linear(9, 128),
            nn.ReLU(),
            nn.Linear(128, 256)
        ).to(self.device)

        # ç‰¹å¾èåˆå±‚
        self.fusion = nn.Sequential(
            nn.Linear(2048 + 256, 512),  # 2048ç»´å›¾åƒ + 256ç»´æ•°å€¼
            nn.ReLU(),
            nn.Dropout(0.3)
        ).to(self.device)

    def encode_entity(self, entity_id, entity_types_df):
        """ç¼–ç å®ä½“çš„å¤šæ¨¡æ€ç‰¹å¾"""
        try:
            # 1. æå–æ•°å€¼ç‰¹å¾
            row = entity_types_df[entity_types_df['entity_id'] == entity_id]
            if row.empty:
                numeric_feat = torch.zeros(9, dtype=torch.float32, device=self.device)
            else:
                numeric_values = []
                for i in range(1, 10):
                    col_name = f'category_{i}_score'
                    if col_name in row.columns:
                        val = row[col_name].values[0]
                        numeric_values.append(float(val) if not pd.isna(val) else 0.0)
                    else:
                        numeric_values.append(0.0)
                numeric_feat = torch.tensor(numeric_values, dtype=torch.float32, device=self.device)

            # 2. æ•°å€¼ç¼–ç 
            numeric_encoded = self.numeric_encoder(numeric_feat.unsqueeze(0)).squeeze(0)

            # 3. ä½¿ç”¨é›¶å›¾åƒç‰¹å¾
            image_feat = torch.zeros(2048, device=self.device)

            # 4. èåˆç‰¹å¾
            combined = torch.cat([image_feat, numeric_encoded], dim=-1)
            fused_feature = self.fusion(combined.unsqueeze(0)).squeeze(0)

            return fused_feature
        except Exception as e:
            print(f"ç¼–ç å®ä½“ {entity_id} æ—¶å‡ºé”™: {e}")
            return torch.zeros(512, device=self.device)


class EntityTypePredictor:
    """å®ä½“ç±»å‹é¢„æµ‹å™¨"""

    def __init__(self, model_path='models/entity_type_predictor_multi_sage.pth'):
        print("=" * 60)
        print("åˆå§‹åŒ–å®ä½“ç±»å‹é¢„æµ‹å™¨...")
        print("=" * 60)

        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # 1. åŠ è½½æ¨¡å‹
        print("1. åŠ è½½è®­ç»ƒå¥½çš„æ¨¡å‹...")
        checkpoint = torch.load(model_path, map_location=self.device)

        # è·å–é…ç½®
        self.model_config = checkpoint['model_config']
        self.entity_to_idx = checkpoint['entity_to_idx']
        self.idx_to_entity = checkpoint['idx_to_entity']
        self.label_encoder = checkpoint['label_encoder']
        self.top_relations = checkpoint['top_relations']
        self.type_to_idx = checkpoint['type_to_idx']
        self.scaler = checkpoint['scaler']

        print(f"æ¨¡å‹é…ç½®:")
        print(f"  ç»“æ„ç‰¹å¾ç»´åº¦: {self.model_config['structural_dim']}")
        print(f"  å¤šæ¨¡æ€ç‰¹å¾ç»´åº¦: {self.model_config['multimodal_dim']}")
        print(f"  éšè—å±‚ç»´åº¦: {self.model_config['h_feats']}")
        print(f"  ç±»åˆ«æ•°é‡: {self.model_config['num_classes']}")

        # åˆ›å»ºæ¨¡å‹ï¼ˆå¿…é¡»ä¸è®­ç»ƒæ—¶å®Œå…¨ä¸€è‡´ï¼‰
        self.model = MultiModalGraphSAGE(
            structural_dim=self.model_config['structural_dim'],
            multimodal_dim=self.model_config['multimodal_dim'],
            h_feats=self.model_config['h_feats'],
            num_classes=self.model_config['num_classes'],
            num_layers=self.model_config['num_layers'],
            dropout=self.model_config['dropout']
        )

        # åŠ è½½æ¨¡å‹å‚æ•°
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.model = self.model.to(self.device)
        self.model.eval()

        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")

        # 2. åŠ è½½å®ä½“ç±»å‹æ•°æ®
        print("\n2. åŠ è½½å®ä½“ç±»å‹æ•°æ®...")
        self.entity_types_df = pd.read_csv('data/FB15KET/Entity_All_typed.csv', encoding='utf-8')

        # é¢„å¤„ç†æ•°å€¼åˆ—
        numeric_cols = [f'category_{i}_score' for i in range(1, 10)]
        for col in numeric_cols:
            if col in self.entity_types_df.columns:
                self.entity_types_df[col] = pd.to_numeric(self.entity_types_df[col], errors='coerce').fillna(0.0)

        print(f"åŠ è½½äº† {len(self.entity_types_df)} ä¸ªå®ä½“ç±»å‹è®°å½•")

        # 3. åˆå§‹åŒ–å¤šæ¨¡æ€ç¼–ç å™¨
        print("\n3. åˆå§‹åŒ–å¤šæ¨¡æ€ç¼–ç å™¨...")
        self.multimodal_encoder = MultiModalEncoder()

        print("âœ… é¢„æµ‹å™¨åˆå§‹åŒ–å®Œæˆ")

    def parse_test_file(self, test_file_path):
        """è§£ææµ‹è¯•æ–‡ä»¶"""
        print(f"\nè§£ææµ‹è¯•æ–‡ä»¶: {test_file_path}")

        test_cases = {}
        current_entity = None
        current_triples = []

        with open(test_file_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        for line in lines:
            line = line.strip()

            # æ£€æµ‹æ–°å®ä½“
            if line.startswith("å®ä½“:") and "(" in line:
                # ä¿å­˜ä¸Šä¸€ä¸ªå®ä½“
                if current_entity and current_triples:
                    test_cases[current_entity] = current_triples

                # æå–æ–°å®ä½“ID
                parts = line.split()
                if len(parts) >= 2:
                    entity_id = parts[1].rstrip('(').rstrip(')')
                    current_entity = entity_id
                    current_triples = []

            # è§£æä¸‰å…ƒç»„
            elif line and current_entity and '\t' in line:
                parts = line.split('\t')
                if len(parts) == 3:
                    current_triples.append((parts[0], parts[1], parts[2]))

        # ä¿å­˜æœ€åä¸€ä¸ªå®ä½“
        if current_entity and current_triples:
            test_cases[current_entity] = current_triples

        print(f"è§£æå®Œæˆ: æ‰¾åˆ° {len(test_cases)} ä¸ªæµ‹è¯•å®ä½“")

        # æ˜¾ç¤ºå‰å‡ ä¸ªå®ä½“ä½œä¸ºç¤ºä¾‹
        print("å‰5ä¸ªå®ä½“ç¤ºä¾‹:")
        for i, (entity_id, triples) in enumerate(list(test_cases.items())[:5]):
            print(f"  {i + 1}. {entity_id}: {len(triples)} ä¸ªå…³ç³»")

        return test_cases

    def extract_structural_features(self, target_entity, neighbor_triples):
        """æå–ç»“æ„ç‰¹å¾"""
        # ç»Ÿè®¡å…³ç³»ä¿¡æ¯
        entity_relations = []
        entity_in_degree = 0
        entity_out_degree = 0

        for h, r, t in neighbor_triples:
            if h == target_entity:
                entity_out_degree += 1
                entity_relations.append(r)
            if t == target_entity:
                entity_in_degree += 1
                entity_relations.append(r)

        # 1. åŸºç¡€ç‰¹å¾
        has_label = 1.0 if target_entity in self.entity_types_df['entity_id'].values else 0.0
        base_features = np.array([
            has_label,
            float(entity_in_degree),
            float(entity_out_degree),
            float(entity_in_degree + entity_out_degree),
            float(len(set(entity_relations))),
            0.0, 0.0, 0.0, 0.0  # å ä½ç¬¦ï¼Œä¸è®­ç»ƒæ—¶ä¸€è‡´
        ], dtype=np.float32)

        # 2. å…³ç³»æ¨¡å¼ç‰¹å¾
        rel_pattern_feat = np.zeros(len(self.top_relations) * 2, dtype=np.float32)

        # ç»Ÿè®¡ä½œä¸ºå¤´å®ä½“å’Œå°¾å®ä½“çš„å…³ç³»åˆ†å¸ƒ
        head_relations = Counter()
        tail_relations = Counter()

        for h, r, t in neighbor_triples:
            if h == target_entity:
                head_relations[r] += 1
            if t == target_entity:
                tail_relations[r] += 1

        total_head = sum(head_relations.values())
        total_tail = sum(tail_relations.values())

        for rel_idx, rel in enumerate(self.top_relations):
            # ä½œä¸ºå¤´å®ä½“çš„å…³ç³»é¢‘ç‡
            if total_head > 0 and rel in head_relations:
                rel_pattern_feat[rel_idx] = head_relations[rel] / total_head

            # ä½œä¸ºå°¾å®ä½“çš„å…³ç³»é¢‘ç‡
            if total_tail > 0 and rel in tail_relations:
                rel_pattern_feat[rel_idx + len(self.top_relations)] = tail_relations[rel] / total_tail

        # 3. é‚»å±…ç±»å‹ç‰¹å¾ï¼ˆé¢„æµ‹æ—¶ç”¨é›¶å‘é‡ï¼‰
        neighbor_type_feat = np.zeros(len(self.type_to_idx), dtype=np.float32)

        # 4. ç»„åˆæ‰€æœ‰ç‰¹å¾
        all_features = np.concatenate([base_features, rel_pattern_feat, neighbor_type_feat])

        # 5. æ ‡å‡†åŒ–ï¼ˆä½¿ç”¨è®­ç»ƒæ—¶çš„scalerï¼‰
        features_scaled = self.scaler.transform(all_features.reshape(1, -1))

        return torch.tensor(features_scaled, dtype=torch.float32).squeeze(0)

    def build_test_graph(self, target_entity, neighbor_triples):
        """ä¸ºæµ‹è¯•å®ä½“æ„å»ºå›¾"""
        # æ”¶é›†æ‰€æœ‰ç›¸å…³å®ä½“
        all_entities = set([target_entity])
        for h, r, t in neighbor_triples:
            all_entities.update([h, t])

        # åˆ›å»ºå®ä½“åˆ°ç´¢å¼•çš„æ˜ å°„
        entity_to_idx = {entity: idx for idx, entity in enumerate(all_entities)}

        # æ„å»ºè¾¹
        src_nodes = []
        dst_nodes = []
        for h, r, t in neighbor_triples:
            src_nodes.append(entity_to_idx[h])
            dst_nodes.append(entity_to_idx[t])

        # åˆ›å»ºDGLå›¾
        num_nodes = len(all_entities)
        g = dgl.graph((torch.tensor(src_nodes), torch.tensor(dst_nodes)),
                      num_nodes=num_nodes)

        # æ·»åŠ è‡ªç¯
        g = dgl.add_self_loop(g)

        return g, entity_to_idx

    def predict_entity_type(self, target_entity, neighbor_triples):
        """é¢„æµ‹å•ä¸ªå®ä½“çš„ç±»å‹"""
        try:
            # 1. æ„å»ºæµ‹è¯•å›¾
            g, entity_to_idx = self.build_test_graph(target_entity, neighbor_triples)
            g = g.to(self.device)

            target_idx = entity_to_idx[target_entity]

            # 2. æå–ç»“æ„ç‰¹å¾
            structural_feat = self.extract_structural_features(target_entity, neighbor_triples)

            # ä¸ºæ‰€æœ‰èŠ‚ç‚¹åˆ›å»ºç‰¹å¾çŸ©é˜µ
            num_nodes = g.num_nodes()
            structural_dim = structural_feat.shape[0]
            all_structural_features = torch.zeros(num_nodes, structural_dim)
            all_structural_features[target_idx] = structural_feat

            # 3. æå–å¤šæ¨¡æ€ç‰¹å¾
            multimodal_feat = self.multimodal_encoder.encode_entity(target_entity, self.entity_types_df)

            # ä¸ºæ‰€æœ‰èŠ‚ç‚¹åˆ›å»ºå¤šæ¨¡æ€ç‰¹å¾çŸ©é˜µ
            multimodal_dim = multimodal_feat.shape[0]
            all_multimodal_features = torch.zeros(num_nodes, multimodal_dim)
            all_multimodal_features[target_idx] = multimodal_feat

            # 4. ç§»åŠ¨åˆ°è®¾å¤‡
            all_structural_features = all_structural_features.to(self.device)
            all_multimodal_features = all_multimodal_features.to(self.device)

            # 5. é¢„æµ‹
            with torch.no_grad():
                logits = self.model(g, all_structural_features, all_multimodal_features)

                # åªè·å–ç›®æ ‡èŠ‚ç‚¹çš„é¢„æµ‹
                target_logits = logits[target_idx:target_idx + 1]
                probabilities = F.softmax(target_logits, dim=-1)
                predicted_class = torch.argmax(probabilities, dim=-1).item()

                # è§£ç ç±»åˆ«
                predicted_type = self.label_encoder.inverse_transform([predicted_class])[0]
                confidence = probabilities[0, predicted_class].item()

                # è·å–top-3é¢„æµ‹
                top3_probs, top3_indices = torch.topk(probabilities[0], k=min(3, len(probabilities[0])))
                top3_types = self.label_encoder.inverse_transform(top3_indices.cpu().numpy())
                top3_confidences = top3_probs.cpu().numpy()

            # å‡†å¤‡ç»“æœ
            result = {
                'entity_id': target_entity,
                'predicted_type': predicted_type,
                'confidence': confidence,
                'top_predictions': [
                    {'type': t, 'confidence': float(c)}
                    for t, c in zip(top3_types, top3_confidences)
                ]
            }

            return result

        except Exception as e:
            print(f"é¢„æµ‹å®ä½“ {target_entity} æ—¶å‡ºé”™: {e}")
            return None

    def batch_predict(self, test_cases):
        """æ‰¹é‡é¢„æµ‹"""
        print(f"\nå¼€å§‹æ‰¹é‡é¢„æµ‹ {len(test_cases)} ä¸ªå®ä½“...")

        results = []
        success_count = 0

        for entity_id, triples in tqdm(test_cases.items(), desc="é¢„æµ‹è¿›åº¦"):
            result = self.predict_entity_type(entity_id, triples)
            if result:
                results.append(result)
                success_count += 1

        print(f"âœ… æ‰¹é‡é¢„æµ‹å®Œæˆ: æˆåŠŸ {success_count}/{len(test_cases)}")
        return results

    def save_predictions(self, predictions, output_file='predictions_fixed.csv'):
        """ä¿å­˜é¢„æµ‹ç»“æœ"""
        if not predictions:
            print("æ²¡æœ‰é¢„æµ‹ç»“æœå¯ä¿å­˜")
            return None

        # å‡†å¤‡æ•°æ®
        data = []
        for pred in predictions:
            row = {
                'entity_id': pred['entity_id'],
                'predicted_type': pred['predicted_type'],
                'confidence': pred['confidence'],
                'top1_type': pred['top_predictions'][0]['type'] if len(pred['top_predictions']) > 0 else '',
                'top1_confidence': pred['top_predictions'][0]['confidence'] if len(pred['top_predictions']) > 0 else 0,
                'top2_type': pred['top_predictions'][1]['type'] if len(pred['top_predictions']) > 1 else '',
                'top2_confidence': pred['top_predictions'][1]['confidence'] if len(pred['top_predictions']) > 1 else 0,
                'top3_type': pred['top_predictions'][2]['type'] if len(pred['top_predictions']) > 2 else '',
                'top3_confidence': pred['top_predictions'][2]['confidence'] if len(pred['top_predictions']) > 2 else 0,
            }
            data.append(row)

        # åˆ›å»ºDataFrame
        df = pd.DataFrame(data)

        # ä¿å­˜åˆ°CSV
        df.to_csv(output_file, index=False, encoding='utf-8')
        print(f"âœ… é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: {output_file}")

        # æ˜¾ç¤ºå‰5æ¡ç»“æœ
        print("\nå‰5æ¡é¢„æµ‹ç»“æœ:")
        print(df.head().to_string())

        return df

    def evaluate_predictions(self, predictions):
        """è¯„ä¼°é¢„æµ‹ç»“æœ"""
        print("\n" + "=" * 60)
        print("è¯„ä¼°é¢„æµ‹ç»“æœ")
        print("=" * 60)

        correct = 0
        total = 0
        evaluation_results = []

        for pred in predictions:
            entity_id = pred['entity_id']

            # æŸ¥æ‰¾çœŸå®æ ‡ç­¾
            true_row = self.entity_types_df[self.entity_types_df['entity_id'] == entity_id]
            if not true_row.empty:
                true_type = true_row.iloc[0]['predicted_category']
                predicted_type = pred['predicted_type']

                total += 1
                is_correct = (true_type == predicted_type)

                if is_correct:
                    correct += 1

                evaluation_results.append({
                    'entity_id': entity_id,
                    'true_type': true_type,
                    'predicted_type': predicted_type,
                    'confidence': pred['confidence'],
                    'is_correct': is_correct
                })

        if total > 0:
            accuracy = correct / total
            print(f"\nğŸ“Š è¯„ä¼°ç»“æœ:")
            print(f"  æ­£ç¡®é¢„æµ‹: {correct}/{total}")
            print(f"  å‡†ç¡®ç‡: {accuracy:.4f}")

            # ä¿å­˜è¯„ä¼°ç»“æœ
            eval_df = pd.DataFrame(evaluation_results)
            eval_df.to_csv('evaluation_results_fixed.csv', index=False, encoding='utf-8')
            print(f"âœ… è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: evaluation_results_fixed.csv")

            return accuracy
        else:
            print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°çœŸå®æ ‡ç­¾ï¼Œæ— æ³•è¯„ä¼°")
            return None


def main():
    """ä¸»å‡½æ•°"""
    print("=" * 80)
    print("å®ä½“ç±»å‹é¢„æµ‹ç³»ç»Ÿ")
    print("=" * 80)

    try:
        # 1. åˆå§‹åŒ–é¢„æµ‹å™¨
        predictor = EntityTypePredictor('models/entity_type_predictor_multi_sage.pth')

        # 2. è§£ææµ‹è¯•æ–‡ä»¶
        test_file = 'data/FB15KET/TEST_PART_DETAILED.txt'
        test_cases = predictor.parse_test_file(test_file)

        if not test_cases:
            print("ä½¿ç”¨ç¤ºä¾‹æ•°æ®...")
            test_cases = {
                '/m/027rn': [
                    ('/m/027rn', '/location/country/form_of_government', '/m/06cx9'),
                    ('/m/01wy61y', '/people/person/nationality', '/m/027rn'),
                ]
            }

        # 3. æ‰¹é‡é¢„æµ‹
        results = predictor.batch_predict(test_cases)

        if results:
            # 4. ä¿å­˜ç»“æœ
            predictor.save_predictions(results)

            # 5. è¯„ä¼°
            accuracy = predictor.evaluate_predictions(results)

            if accuracy is not None:
                print(f"\nğŸ¯ æœ€ç»ˆå‡†ç¡®ç‡: {accuracy:.4f}")

                # æ˜¾ç¤ºç½®ä¿¡åº¦ç»Ÿè®¡
                confidences = [r['confidence'] for r in results]
                print(f"\nğŸ“ˆ ç½®ä¿¡åº¦ç»Ÿè®¡:")
                print(f"  å¹³å‡ç½®ä¿¡åº¦: {np.mean(confidences):.4f}")
                print(f"  ä¸­ä½æ•°ç½®ä¿¡åº¦: {np.median(confidences):.4f}")
                print(f"  æœ€é«˜ç½®ä¿¡åº¦: {np.max(confidences):.4f}")
                print(f"  æœ€ä½ç½®ä¿¡åº¦: {np.min(confidences):.4f}")
        else:
            print("âŒ æ²¡æœ‰æˆåŠŸçš„é¢„æµ‹ç»“æœ")

    except Exception as e:
        print(f"ç¨‹åºæ‰§è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()