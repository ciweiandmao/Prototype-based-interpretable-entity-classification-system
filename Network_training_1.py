import math
import pandas as pd
import numpy as np
import torch
import dgl
import os
from collections import Counter, defaultdict
import matplotlib.pyplot as plt
import seaborn as sns
import torch.nn as nn
import torch.nn.functional as F
import dgl.nn as dglnn
from sklearn.preprocessing import StandardScaler
import warnings

# å¿½ç•¥æ‰€æœ‰ UserWarning
warnings.filterwarnings("ignore", category=UserWarning)


class EnhancedFeatureExtractor:
    """å¢å¼ºç‰¹å¾æå–å™¨"""

    def extract_structural_features(self, entity_id, triplets_by_entity):
        """æå–ç»“æ„ç‰¹å¾"""
        if entity_id not in triplets_by_entity:
            return [0.0] * 5

        relations = triplets_by_entity[entity_id]

        # 1. åº¦ä¸­å¿ƒæ€§ç‰¹å¾
        out_degree = sum(1 for d, _, _ in relations if d == 'out')
        in_degree = sum(1 for d, _, _ in relations if d == 'in')
        total_degree = len(relations)

        # 2. å…³ç³»å¤šæ ·æ€§
        rel_types = set(rel for _, rel, _ in relations)
        rel_diversity = len(rel_types) / (total_degree + 1e-8)

        # 3. é‚»å±…ç±»åˆ«åˆ†å¸ƒï¼ˆç®€åŒ–ï¼‰
        # è¿™é‡Œå¯ä»¥æ·»åŠ é‚»å±…çš„ç±»åˆ«ç»Ÿè®¡

        return [
            out_degree / 100.0,  # å½’ä¸€åŒ–
            in_degree / 100.0,
            total_degree / 200.0,
            rel_diversity,
            len(rel_types) / 50.0  # å…³ç³»ç±»å‹æ•°é‡å½’ä¸€åŒ–
        ]

    def extract_text_features(self, entity_id, entity_info):
        """æå–æ–‡æœ¬ç‰¹å¾ï¼ˆå¦‚æœæœ‰ï¼‰"""
        # å¦‚æœæœ‰å®ä½“æè¿°æ–‡æœ¬ï¼Œå¯ä»¥ä½¿ç”¨BERTæå–ç‰¹å¾
        # è¿™é‡Œå…ˆè¿”å›ç©ºç‰¹å¾
        return [0.0] * 5  # å ä½ç¬¦


class DataAugmenter:
    """æ•°æ®å¢å¼ºå™¨"""

    def __init__(self, triplets_by_entity):
        self.triplets_by_entity = triplets_by_entity

    def augment_by_relation(self, entity_id, max_neighbors=5):
        """é€šè¿‡å…³ç³»è¿›è¡Œæ•°æ®å¢å¼º"""
        if entity_id not in self.triplets_by_entity:
            return []

        relations = self.triplets_by_entity[entity_id]

        # æ‰¾åˆ°ä¸è¯¥å®ä½“æœ‰ç›¸åŒå…³ç³»çš„å…¶ä»–å®ä½“
        augmented_samples = []

        for direction, rel, neighbor in relations:
            # æ‰¾åˆ°æœ‰ç›¸åŒå…³ç³»çš„å…¶ä»–å®ä½“
            similar_entities = self.find_similar_entities(entity_id, rel, direction)

            for sim_entity in similar_entities[:max_neighbors]:
                augmented_samples.append({
                    'source': entity_id,
                    'target': sim_entity,
                    'relation': rel,
                    'direction': direction,
                    'type': 'relation_augment'
                })

        return augmented_samples

    def find_similar_entities(self, entity_id, relation, direction, max_results=10):
        """æ‰¾åˆ°æœ‰ç›¸åŒå…³ç³»çš„å®ä½“"""
        similar = []

        # éå†æ‰€æœ‰å®ä½“ï¼Œæ‰¾åˆ°æœ‰ç›¸åŒå…³ç³»æ¨¡å¼çš„
        for other_entity, relations in self.triplets_by_entity.items():
            if other_entity == entity_id:
                continue

            # æ£€æŸ¥æ˜¯å¦æœ‰ç›¸åŒçš„å…³ç³»
            has_same_rel = any(
                r[1] == relation and r[0] == direction
                for r in relations[:20]  # åªæ£€æŸ¥å‰20ä¸ªå…³ç³»
            )

            if has_same_rel:
                similar.append(other_entity)
                if len(similar) >= max_results:
                    break

        return similar


class EnsembleModel:
    """é›†æˆæ¨¡å‹"""

    def __init__(self, model_configs, device='cuda'):
        self.models = []
        self.device = device

        for config in model_configs:
            model = EnhancedFB15KETXGradNet(**config)
            model = model.to(device)
            self.models.append(model)

    def train_ensemble(self, g, n_epochs=50):
        """è®­ç»ƒé›†æˆæ¨¡å‹"""
        for i, model in enumerate(self.models):
            print(f"\nè®­ç»ƒç¬¬ {i + 1}/{len(self.models)} ä¸ªæ¨¡å‹...")
            trainer = EnhancedTrainer(model, g, self.device)
            trainer.train(epochs=n_epochs)

            # ä¿å­˜æ¨¡å‹
            torch.save(model.state_dict(), f'ensemble_model_{i}.pth')

    def predict(self, features):
        """é›†æˆé¢„æµ‹"""
        all_probs = []

        for model in self.models:
            model.eval()
            with torch.no_grad():
                embeddings = model(features)
                probs, _, _ = model.classify(embeddings, torch.arange(features.size(0)).to(self.device))
                if probs is not None:
                    all_probs.append(probs)

        if all_probs:
            # å¹³å‡æ¦‚ç‡
            avg_probs = torch.stack(all_probs).mean(dim=0)
            _, predicted = torch.max(avg_probs, dim=1)
            return avg_probs, predicted
        else:
            return None, None


def improved_training_pipeline(g):
    """æ”¹è¿›çš„è®­ç»ƒæµç¨‹"""
    print("=" * 80)
    print("FB15KETå®ä½“åˆ†ç±»ç³»ç»Ÿ - æ”¹è¿›è®­ç»ƒæµç¨‹")
    print("=" * 80)

    # 1. æ„å»ºå¢å¼ºç‰¹å¾
    print("\n[1] æ„å»ºå¢å¼ºç‰¹å¾...")
    enhanced_features = build_enhanced_features()

    # 2. é‡æ–°æ„å»ºå›¾ï¼ˆä½¿ç”¨å¢å¼ºç‰¹å¾ï¼‰
    print("\n[2] é‡æ–°æ„å»ºå›¾ï¼ˆå¢å¼ºç‰¹å¾ï¼‰...")
    # è¿™é‡Œéœ€è¦ä¿®æ”¹build_heterogeneous_graphå‡½æ•°ä»¥ä½¿ç”¨enhanced_features

    # 3. è®­ç»ƒå¢å¼ºæ¨¡å‹
    print("\n[3] è®­ç»ƒå¢å¼ºæ¨¡å‹...")
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # æ–°ç‰¹å¾ç»´åº¦ = 9ï¼ˆåŸå§‹å¾—åˆ†ï¼‰+ 5ï¼ˆç»“æ„ç‰¹å¾ï¼‰= 14
    feature_dim = 14
    hidden_dim = 256
    out_dim = 128

    model = EnhancedFB15KETXGradNet(
        feature_dim=feature_dim,
        hidden_dim=hidden_dim,
        out_dim=out_dim,
        num_classes=9,
        num_prototypes_per_class=3,
        dropout_rate=0.5
    )

    trainer = EnhancedTrainer(model, g, device=device)

    # è®­ç»ƒæ›´å¤šepoch
    train_losses, val_accuracies = trainer.train(
        epochs=150,  # æ›´å¤šepoch
        lr=0.001,
        weight_decay=1e-4,
        warmup_epochs=15,
        patience=40
    )

    # 4. æµ‹è¯•
    print("\n[4] æµ‹è¯•æ”¹è¿›æ¨¡å‹...")
    results = trainer.test(save_results=True)

    # 5. åˆ†æ
    print("\n[5] æ€§èƒ½åˆ†æ...")
    analyze_results(results, train_losses, val_accuracies)

    return results


def analyze_results(results, train_losses, val_accuracies):
    """åˆ†æè®­ç»ƒç»“æœ"""
    print("\næ€§èƒ½åˆ†ææŠ¥å‘Š:")
    print("-" * 60)

    train_acc = results.get('train_acc', 0)
    val_acc = results.get('valid_acc', 0)
    test_acc = results.get('test_acc', 0)

    print(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {train_acc:.4f}")
    print(f"éªŒè¯é›†å‡†ç¡®ç‡: {val_acc:.4f}")
    print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f}")

    # åˆ†æè¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆ
    if train_acc > val_acc + 0.05:
        print("âš ï¸  å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆ (è®­ç»ƒé›† >> éªŒè¯é›†)")
        print("   å»ºè®®: å¢åŠ dropout, æ•°æ®å¢å¼º, æ—©åœ")
    elif train_acc < val_acc:
        print("âš ï¸  å¯èƒ½å­˜åœ¨æ¬ æ‹Ÿåˆ (è®­ç»ƒé›† < éªŒè¯é›†)")
        print("   å»ºè®®: å¢åŠ æ¨¡å‹å¤æ‚åº¦, è®­ç»ƒæ›´å¤šepoch, æ•°æ®å¢å¼º")
    else:
        print("âœ“ è®­ç»ƒé›†å’ŒéªŒè¯é›†æ€§èƒ½å¹³è¡¡")

    # æ³›åŒ–èƒ½åŠ›
    if abs(val_acc - test_acc) < 0.02:
        print("âœ“ æ³›åŒ–èƒ½åŠ›è‰¯å¥½ (éªŒè¯é›† â‰ˆ æµ‹è¯•é›†)")
    else:
        print("âš ï¸  æ³›åŒ–èƒ½åŠ›æœ‰å¾…æå‡")

    # ç»å¯¹æ€§èƒ½
    if test_acc > 0.7:
        print("ğŸ‰ æ€§èƒ½ä¼˜ç§€ (>70%)")
    elif test_acc > 0.6:
        print("ğŸ‘ æ€§èƒ½è‰¯å¥½ (60-70%)")
    elif test_acc > 0.5:
        print("ğŸ‘Œ æ€§èƒ½ä¸€èˆ¬ (50-60%)")
    else:
        print("ğŸ”§ éœ€è¦å¤§å¹…æ”¹è¿› (<50%)")

def build_enhanced_features():
    """æ„å»ºå¢å¼ºç‰¹å¾"""
    # åŠ è½½ä¸‰å…ƒç»„æ•°æ®
    triplets_by_entity = defaultdict(list)
    for file_name in ['train.txt', 'valid.txt', 'test.txt']:
        file_path = f'data/FB15KET/{file_name}'
        if os.path.exists(file_path):
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    parts = line.strip().split('\t')
                    if len(parts) == 3:
                        h, r, t = parts
                        triplets_by_entity[h].append(('out', r, t))
                        triplets_by_entity[t].append(('in', r, h))

    # åŠ è½½å®ä½“æ•°æ®
    entity_df = pd.read_csv('data/FB15KET/Entity_All_typed.csv')

    # åˆ›å»ºç‰¹å¾æå–å™¨
    extractor = EnhancedFeatureExtractor()

    enhanced_features = {}
    for _, row in entity_df.iterrows():
        eid = row['entity_id']

        # åŸºæœ¬ç‰¹å¾ï¼š9ä¸ªç±»åˆ«å¾—åˆ†
        base_features = [row[f'category_{i}_score'] for i in range(1, 10)]

        # ç»“æ„ç‰¹å¾
        structural_features = extractor.extract_structural_features(eid, triplets_by_entity)

        # ç»„åˆç‰¹å¾
        all_features = base_features + structural_features

        enhanced_features[eid] = all_features

    return enhanced_features
class FB15KETDataLoader:
    def __init__(self, data_dir='data/FB15KET'):
        self.data_dir = data_dir
        self.entity_type_path = os.path.join(data_dir, 'Entity_All_typed.csv')
        self.train_path = os.path.join(data_dir, 'xunlian.txt')
        #self.valid_path = os.path.join(data_dir, 'valid.txt')
        #self.test_path = os.path.join(data_dir, 'test.txt')

        # 9ä¸ªç±»åˆ«åç§°æ˜ å°„
        self.category_names = {
            1: "äººç‰©å’Œç”Ÿå‘½ï¼ˆPerson & Lifeï¼‰",
            2: "ç»„ç»‡ä¸æœºæ„ï¼ˆOrganizationï¼‰",
            3: "åœ°ç‚¹ä¸åœ°ç†ï¼ˆLocationï¼‰",
            4: "åˆ›ä½œä¸å¨±ä¹ä½œå“ï¼ˆCreative Workï¼‰",
            5: "äº‹ä»¶ä¸æ´»åŠ¨ï¼ˆEventï¼‰",
            6: "å­¦ç§‘ä¸æ¦‚å¿µï¼ˆConcept & Subjectï¼‰",
            7: "ç‰©å“ä¸äº§å“ï¼ˆProduct & Objectï¼‰",
            8: "å±æ€§ä¸åº¦é‡ï¼ˆAttribute & Measurementï¼‰",
            9: "å…¶ä»–ï¼ˆOthersï¼‰"
        }

    def analyze_data_quality(self):
        """åˆ†ææ•°æ®è´¨é‡å’Œåˆ†å¸ƒ"""
        print("=" * 60)
        print("FB15KET æ•°æ®é›†è´¨é‡åˆ†æ")
        print("=" * 60)

        # 1. åˆ†æå®ä½“ç±»å‹æ–‡ä»¶
        print("\n1. å®ä½“ç±»å‹æ–‡ä»¶åˆ†æ:")
        entity_df = pd.read_csv(self.entity_type_path)
        print(f"  å®ä½“æ€»æ•°: {len(entity_df)}")
        print(f"  åˆ—ä¿¡æ¯: {entity_df.columns.tolist()}")

        # ç±»åˆ«åˆ†å¸ƒ
        if 'predicted_category' in entity_df.columns:
            category_counts = entity_df['predicted_category'].value_counts()
            print(f"\n  ç±»åˆ«åˆ†å¸ƒ:")
            for cat_id, count in category_counts.items():
                cat_name = self.category_names.get(int(cat_id), f"æœªçŸ¥ç±»åˆ«{cat_id}")
                print(f"    {cat_id}: {cat_name} - {count}ä¸ªå®ä½“ ({count / len(entity_df) * 100:.2f}%)")

        # 2. åˆ†æä¸‰å…ƒç»„æ–‡ä»¶
        print("\n2. ä¸‰å…ƒç»„æ–‡ä»¶åˆ†æ:")
        file_info = []
        for file_name in ['xunlian.txt']:
            file_path = os.path.join(self.data_dir, file_name)
            with open(file_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
                triplets = [line.strip().split('\t') for line in lines]
                unique_entities = set()
                unique_relations = set()
                for h, r, t in triplets:
                    unique_entities.add(h)
                    unique_entities.add(t)
                    unique_relations.add(r)

                file_info.append({
                    'file': file_name,
                    'triplets': len(triplets),
                    'entities': len(unique_entities),
                    'relations': len(unique_relations)
                })

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        for info in file_info:
            print(f"  {info['file']}:")
            print(f"    ä¸‰å…ƒç»„æ•°é‡: {info['triplets']:,}")
            print(f"    å”¯ä¸€å®ä½“æ•°: {info['entities']:,}")
            print(f"    å”¯ä¸€å…³ç³»æ•°: {info['relations']:,}")

        # 3. åˆå¹¶åˆ†æ
        print("\n3. åˆå¹¶åˆ†æ:")
        all_entities = set()
        all_relations = set()
        total_triplets = 0

        for file_name in ['xunlian.txt']:
            file_path = os.path.join(self.data_dir, file_name)
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    h, r, t = line.strip().split('\t')
                    all_entities.add(h)
                    all_entities.add(t)
                    all_relations.add(r)
                    total_triplets += 1

        print(f"  æ€»ä¸‰å…ƒç»„æ•°: {total_triplets:,}")
        print(f"  æ€»å”¯ä¸€å®ä½“æ•°: {len(all_entities):,}")
        print(f"  æ€»å”¯ä¸€å…³ç³»æ•°: {len(all_relations):,}")

        # 4. æ£€æŸ¥å®ä½“ç±»å‹è¦†ç›–
        print("\n4. å®ä½“ç±»å‹è¦†ç›–åˆ†æ:")
        typed_entities = set(entity_df['entity_id'].unique())
        all_entity_ids = all_entities
        typed_count = len(typed_entities & all_entity_ids)
        untyped_count = len(all_entity_ids - typed_entities)

        print(f"  æœ‰ç±»å‹æ ‡æ³¨çš„å®ä½“: {typed_count:,} ({typed_count / len(all_entity_ids) * 100:.2f}%)")
        print(f"  æ— ç±»å‹æ ‡æ³¨çš„å®ä½“: {untyped_count:,} ({untyped_count / len(all_entity_ids) * 100:.2f}%)")

        # 5. å…³ç³»é¢‘ç‡åˆ†æ
        print("\n5. å…³ç³»é¢‘ç‡åˆ†æ (Top 20):")
        relation_counts = Counter()
        for file_name in ['xunlian.txt']:  # åªåˆ†æè®­ç»ƒé›†çš„å…³ç³»åˆ†å¸ƒ
            file_path = os.path.join(self.data_dir, file_name)
            with open(file_path, 'r', encoding='utf-8') as f:
                for line in f:
                    _, r, _ = line.strip().split('\t')
                    relation_counts[r] += 1

        print("  æœ€å¸¸è§çš„å…³ç³»:")
        for i, (rel, count) in enumerate(relation_counts.most_common(20)):
            print(f"    {i + 1:2d}. {rel}: {count:,}")

        # 6. å¯è§†åŒ–ç±»åˆ«åˆ†å¸ƒ
        if 'predicted_category' in entity_df.columns:
            plt.figure(figsize=(12, 6))
            category_dist = entity_df['predicted_category'].value_counts().sort_index()
            categories = [f"{idx}\n{self.category_names.get(idx, '')[:10]}..." for idx in category_dist.index]

            plt.bar(range(len(categories)), category_dist.values)
            plt.xticks(range(len(categories)), categories, rotation=45, ha='right')
            plt.title('å®ä½“ç±»åˆ«åˆ†å¸ƒ')
            plt.xlabel('ç±»åˆ«')
            plt.ylabel('å®ä½“æ•°é‡')
            plt.tight_layout()
            plt.savefig('processed_data/category_distribution.png', dpi=150, bbox_inches='tight')
            plt.close()

            print(f"\n  ç±»åˆ«åˆ†å¸ƒå›¾å·²ä¿å­˜åˆ°: processed_data/category_distribution.png")

        # 7. æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
        print("\n6. æ•°æ®å®Œæ•´æ€§æ£€æŸ¥:")
        missing_files = []
        for file_path in [self.entity_type_path, self.train_path]:
            if not os.path.exists(file_path):
                missing_files.append(file_path)

        if missing_files:
            print(f"  è­¦å‘Š: ä»¥ä¸‹æ–‡ä»¶ä¸å­˜åœ¨: {missing_files}")
        else:
            print("  æ‰€æœ‰å¿…éœ€æ–‡ä»¶éƒ½å­˜åœ¨")

        print("\n" + "=" * 60)

        # è¿”å›ç»Ÿè®¡ä¿¡æ¯
        return {
            'entity_count': len(all_entities),
            'relation_count': len(all_relations),
            'triplet_count': total_triplets,
            'typed_entity_count': typed_count,
            'untyped_entity_count': untyped_count,
            'category_distribution': category_counts if 'predicted_category' in entity_df.columns else None
        }


class FB15KETGraphBuilder:
    def __init__(self, data_dir='data/FB15KET'):
        self.data_dir = data_dir
        self.data_loader = FB15KETDataLoader(data_dir)

    def load_all_data(self):
        """åŠ è½½æ‰€æœ‰æ•°æ®ï¼ˆä¿®å¤ç´¢å¼•è¶Šç•Œé—®é¢˜ï¼‰"""
        print("æ­£åœ¨åŠ è½½æ•°æ®...")

        try:
            # 1. åŠ è½½å®ä½“ç±»å‹ä¿¡æ¯
            entity_df = pd.read_csv(os.path.join(self.data_dir, 'Entity_All_typed.csv'))

            # æå–ç±»åˆ«å¾—åˆ†ä½œä¸ºç‰¹å¾ï¼Œå¹¶ç¡®å®šä¸»è¦ç±»åˆ«
            score_cols = [f'category_{i}_score' for i in range(1, 10)]

            # åˆ›å»ºå®ä½“åˆ°ç‰¹å¾çš„æ˜ å°„
            entity_features = {}
            entity_labels = {}

            for _, row in entity_df.iterrows():
                eid = row['entity_id']
                # ç‰¹å¾ï¼š9ä¸ªç±»åˆ«çš„å¾—åˆ†
                features = [float(row[col]) for col in score_cols]
                entity_features[eid] = features

                # æ ‡ç­¾ï¼šå¾—åˆ†æœ€é«˜çš„ç±»åˆ«ï¼ˆ1-9ï¼‰
                if 'predicted_category' in row and not pd.isna(row['predicted_category']):
                    entity_labels[eid] = int(row['predicted_category'])
                else:
                    # å¦‚æœæ²¡æœ‰é¢„æµ‹ç±»åˆ«ï¼Œä½¿ç”¨å¾—åˆ†æœ€é«˜çš„
                    scores = [float(row[col]) for col in score_cols]
                    entity_labels[eid] = np.argmax(scores) + 1

            # 2. åŠ è½½æ‰€æœ‰ä¸‰å…ƒç»„
            all_triplets = []
            entity_set = set()
            relation_set = set()

            for file_name in ['xunlian.txt']:
                file_path = os.path.join(self.data_dir, file_name)
                if os.path.exists(file_path):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        for line in f:
                            parts = line.strip().split('\t')
                            if len(parts) == 3:
                                h, r, t = parts
                                all_triplets.append((h, r, t))
                                entity_set.add(h)
                                entity_set.add(t)
                                relation_set.add(r)

            # æ£€æŸ¥å®ä½“æ•°é‡
            print(f"åŠ è½½å®Œæˆ: {len(entity_set)} ä¸ªå®ä½“, {len(relation_set)} ç§å…³ç³», {len(all_triplets)} ä¸ªä¸‰å…ƒç»„")

            # 3. æ£€æŸ¥å“ªäº›å®ä½“æœ‰ç‰¹å¾
            entities_with_features = set(entity_features.keys())
            entities_without_features = entity_set - entities_with_features

            print(f"  æœ‰ç‰¹å¾çš„å®ä½“: {len(entities_with_features)}")
            print(f"  æ— ç‰¹å¾çš„å®ä½“: {len(entities_without_features)}")

            if entities_without_features:
                print(f"  å‰10ä¸ªæ— ç‰¹å¾å®ä½“: {list(entities_without_features)[:10]}")
                # ä¸ºæ— ç‰¹å¾å®ä½“åˆ›å»ºé»˜è®¤ç‰¹å¾ï¼ˆå…¨é›¶ï¼‰
                for eid in entities_without_features:
                    entity_features[eid] = [0.0] * 9

            # 4. åˆ’åˆ†æ•°æ®é›†
            # è¯»å–æ¯ä¸ªæ–‡ä»¶ä¸­çš„å®ä½“ï¼Œç”¨äºæ•°æ®é›†åˆ’åˆ†
            train_entities = set()
            valid_entities = set()
            test_entities = set()

            file_mapping = {
                'xunlian.txt': train_entities,
               # 'valid.txt': valid_entities,
               # 'test.txt': test_entities
            }

            for file_name, entity_set_ref in file_mapping.items():
                file_path = os.path.join(self.data_dir, file_name)
                if os.path.exists(file_path):
                    with open(file_path, 'r', encoding='utf-8') as f:
                        for line in f:
                            parts = line.strip().split('\t')
                            if len(parts) == 3:
                                h, r, t = parts
                                entity_set_ref.add(h)
                                entity_set_ref.add(t)

            # 5. ç¡®ä¿æ‰€æœ‰å®ä½“éƒ½æœ‰æ ‡ç­¾ï¼ˆå¦‚æœæ²¡æœ‰ï¼Œä½¿ç”¨é»˜è®¤æ ‡ç­¾1ï¼‰
            for eid in entity_set:
                if eid not in entity_labels:
                    entity_labels[eid] = 1  # é»˜è®¤ç±»åˆ«ä¸º1ï¼ˆäººç‰©å’Œç”Ÿå‘½ï¼‰

            return {
                'entity_features': entity_features,
                'entity_labels': entity_labels,
                'all_triplets': all_triplets,
                'all_entities': list(entity_set),
                'all_relations': list(relation_set),
                'train_entities': train_entities,
                'valid_entities': valid_entities,
                'test_entities': test_entities,
                'entities_without_features': list(entities_without_features)
            }

        except Exception as e:
            print(f"æ•°æ®åŠ è½½å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None

    def build_heterogeneous_graph(self, use_relation_types=True, max_relations=30):
        """æ„å»ºå¼‚æ„å›¾ï¼ˆä¿®å¤ç´¢å¼•è¶Šç•Œé—®é¢˜ï¼‰"""
        print("\næ­£åœ¨æ„å»ºå¼‚æ„å›¾...")

        data = self.load_all_data()

        if not data:
            print("æ•°æ®åŠ è½½å¤±è´¥")
            return None, None, None

        # 1. åˆ›å»ºIDæ˜ å°„
        print("åˆ›å»ºå®ä½“å’Œå…³ç³»IDæ˜ å°„...")
        entity_id_map = {eid: idx for idx, eid in enumerate(data['all_entities'])}
        relation_id_map = {rid: idx for idx, rid in enumerate(data['all_relations'][:max_relations])}

        print(f"  å®ä½“æ˜ å°„: {len(entity_id_map)} ä¸ªå®ä½“")
        print(f"  å…³ç³»æ˜ å°„: {len(relation_id_map)} ç§å…³ç³»")

        # 2. æ„å»ºè¾¹æ•°æ®
        print("æ„å»ºè¾¹æ•°æ®...")

        if use_relation_types and len(data['all_relations']) > 0:
            # ä½¿ç”¨å¼‚æ„è¾¹
            edge_dict = {}

            # ç»Ÿè®¡å…³ç³»é¢‘ç‡
            relation_counts = Counter()
            for h, r, t in data['all_triplets']:
                if r in relation_id_map:  # åªä½¿ç”¨å‰max_relationsç§å…³ç³»
                    relation_counts[r] += 1

            print(f"ä½¿ç”¨ {len(relation_counts)} ç§å…³ç³»ç±»å‹")

            # ä¸ºæ¯ç§å…³ç³»ç±»å‹åˆ›å»ºè¾¹
            for h, r, t in data['all_triplets']:
                if r in relation_id_map:
                    rel_key = f'rel_{relation_id_map[r]}'
                    if rel_key not in edge_dict:
                        edge_dict[rel_key] = ([], [])
                    # ç¡®ä¿å®ä½“åœ¨æ˜ å°„ä¸­
                    if h in entity_id_map and t in entity_id_map:
                        edge_dict[rel_key][0].append(entity_id_map[h])
                        edge_dict[rel_key][1].append(entity_id_map[t])

            # è½¬æ¢ä¸ºDGLå›¾æ ¼å¼
            hetero_edges = {}
            for rel_key, (src, dst) in edge_dict.items():
                if src and dst:  # ç¡®ä¿è¾¹ä¸ä¸ºç©º
                    hetero_edges[('entity', rel_key, 'entity')] = (torch.tensor(src), torch.tensor(dst))

            if hetero_edges:
                g = dgl.heterograph(hetero_edges)
                print(f"æ„å»ºå¼‚æ„å›¾å®Œæˆ: {g}")
            else:
                print("è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„è¾¹ï¼Œåˆ›å»ºç©ºå›¾")
                # åˆ›å»ºç©ºå›¾
                g = dgl.heterograph({
                    ('entity', 'rel_0', 'entity'): ([0], [0])  # è‡³å°‘ä¸€æ¡è¾¹
                })

        else:
            # ç®€åŒ–ï¼šæ‰€æœ‰å…³ç³»è§†ä¸ºåŒä¸€ç§ç±»å‹
            print("ä½¿ç”¨åŒæ„å›¾...")
            src_nodes, dst_nodes = [], []
            edge_count = 0

            for h, r, t in data['all_triplets']:
                # ç¡®ä¿å®ä½“åœ¨æ˜ å°„ä¸­
                if h in entity_id_map and t in entity_id_map:
                    src_nodes.append(entity_id_map[h])
                    dst_nodes.append(entity_id_map[t])
                    edge_count += 1

            print(f"  æœ‰æ•ˆè¾¹æ•°: {edge_count}/{len(data['all_triplets'])}")

            if src_nodes and dst_nodes:
                g = dgl.graph((src_nodes, dst_nodes))
                print(f"æ„å»ºåŒæ„å›¾å®Œæˆ: {g}")
            else:
                print("è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„è¾¹ï¼Œåˆ›å»ºç©ºå›¾")
                g = dgl.graph(([0], [0]))

        # 3. æ£€æŸ¥å›¾èŠ‚ç‚¹æ•°é‡å¹¶è°ƒæ•´
        num_entities = len(data['all_entities'])
        print(f"  å®ä½“æ€»æ•°: {num_entities}")
        print(f"  å›¾èŠ‚ç‚¹æ•°: {g.num_nodes()}")

        # å¦‚æœå›¾èŠ‚ç‚¹æ•°å°äºå®ä½“æ•°ï¼Œæ·»åŠ å­¤ç«‹èŠ‚ç‚¹
        if g.num_nodes() < num_entities:
            print(f"  æ·»åŠ  {num_entities - g.num_nodes()} ä¸ªå­¤ç«‹èŠ‚ç‚¹")
            g = dgl.add_nodes(g, num_entities - g.num_nodes())

        # 4. æ·»åŠ èŠ‚ç‚¹ç‰¹å¾
        print("æ·»åŠ èŠ‚ç‚¹ç‰¹å¾...")
        node_feat_dim = 10
        node_features = np.zeros((num_entities, node_feat_dim))

        # æ ‡ç­¾
        node_labels = np.full(num_entities, -1, dtype=int)  # -1è¡¨ç¤ºæ— æ ‡ç­¾

        feature_count = 0
        labeled_count = 0

        for eid, idx in entity_id_map.items():
            # è°ƒè¯•ï¼šæ£€æŸ¥ç´¢å¼•æ˜¯å¦åœ¨æœ‰æ•ˆèŒƒå›´å†…
            if idx >= num_entities:
                print(f"  è­¦å‘Š: ç´¢å¼•è¶Šç•Œ: eid={eid}, idx={idx}, num_entities={num_entities}")
                continue

            if eid in data['entity_features']:
                # å‰9ç»´ï¼šç±»åˆ«å¾—åˆ†
                scores = data['entity_features'][eid]
                if len(scores) == 9:
                    node_features[idx, :9] = scores
                else:
                    print(f"  è­¦å‘Š: å®ä½“ {eid} çš„ç‰¹å¾é•¿åº¦é”™è¯¯: {len(scores)}")
                    node_features[idx, :min(9, len(scores))] = scores[:9]

                # ç¬¬10ç»´ï¼šç‰¹å¾å­˜åœ¨æ ‡å¿—
                node_features[idx, 9] = 1.0
                feature_count += 1

                if eid in data['entity_labels']:
                    # æ ‡ç­¾ï¼š0-8ï¼Œå¯¹åº”ç±»åˆ«1-9
                    label_val = data['entity_labels'][eid]
                    if 1 <= label_val <= 9:
                        node_labels[idx] = label_val - 1
                        labeled_count += 1
                    else:
                        print(f"  è­¦å‘Š: å®ä½“ {eid} çš„æ ‡ç­¾å€¼æ— æ•ˆ: {label_val}")
                        node_labels[idx] = 0  # é»˜è®¤ç±»åˆ«

        print(f"  æœ‰ç‰¹å¾çš„å®ä½“: {feature_count}/{num_entities}")
        print(f"  æœ‰æ ‡ç­¾çš„å®ä½“: {labeled_count}/{num_entities}")

        # æ£€æŸ¥æ˜¯å¦æœ‰æœªåˆ†é…çš„æ ‡ç­¾
        unlabeled_count = (node_labels == -1).sum()
        if unlabeled_count > 0:
            print(f"  è­¦å‘Š: {unlabeled_count} ä¸ªå®ä½“æ²¡æœ‰æ ‡ç­¾ï¼Œåˆ†é…é»˜è®¤æ ‡ç­¾")
            node_labels[node_labels == -1] = 0  # é»˜è®¤ç±»åˆ«

        # 5. æ·»åŠ æ•°æ®é›†åˆ’åˆ†æ©ç 
        print("æ·»åŠ æ•°æ®é›†åˆ’åˆ†æ©ç ...")
        train_mask = np.zeros(num_entities, dtype=bool)
        valid_mask = np.zeros(num_entities, dtype=bool)
        test_mask = np.zeros(num_entities, dtype=bool)

        train_entity_count = 0
        valid_entity_count = 0
        test_entity_count = 0

        for eid, idx in entity_id_map.items():
            if idx >= num_entities:
                continue

            if eid in data['train_entities']:
                train_mask[idx] = True
                train_entity_count += 1
            if eid in data['valid_entities']:
                valid_mask[idx] = True
                valid_entity_count += 1
            if eid in data['test_entities']:
                test_mask[idx] = True
                test_entity_count += 1

        # è®­ç»ƒé›†ä¸­æœ‰æ ‡ç­¾çš„èŠ‚ç‚¹
        labeled_train_mask = train_mask & (node_labels != -1)

        # 6. æ·»åŠ åˆ°å›¾æ•°æ®ä¸­
        print(f"  æ·»åŠ ç‰¹å¾: çŸ©é˜µå½¢çŠ¶={node_features.shape}, å›¾èŠ‚ç‚¹={g.num_nodes()}")

        # å†æ¬¡æ£€æŸ¥ç»´åº¦
        if node_features.shape[0] != g.num_nodes():
            print(f"  é”™è¯¯: ç‰¹å¾çŸ©é˜µè¡Œæ•°({node_features.shape[0]}) != å›¾èŠ‚ç‚¹æ•°({g.num_nodes()})")
            # è°ƒæ•´å›¾èŠ‚ç‚¹æ•°
            if g.num_nodes() < node_features.shape[0]:
                g = dgl.add_nodes(g, node_features.shape[0] - g.num_nodes())
            else:
                # è¿™ä¸åº”è¯¥å‘ç”Ÿï¼Œä½†å¦‚æœå‘ç”Ÿå°±æˆªæ–­
                node_features = node_features[:g.num_nodes()]
                node_labels = node_labels[:g.num_nodes()]
                train_mask = train_mask[:g.num_nodes()]
                valid_mask = valid_mask[:g.num_nodes()]
                test_mask = test_mask[:g.num_nodes()]
                labeled_train_mask = labeled_train_mask[:g.num_nodes()]

        # éªŒè¯ç´¢å¼•èŒƒå›´
        print(f"  éªŒè¯: ç‰¹å¾çŸ©é˜µç´¢å¼•èŒƒå›´ 0-{node_features.shape[0] - 1}")
        print(f"  éªŒè¯: æ ‡ç­¾çŸ©é˜µç´¢å¼•èŒƒå›´ 0-{len(node_labels) - 1}")

        # æ·»åŠ è‡ªå¾ªç¯å¤„ç†é›¶å…¥åº¦èŠ‚ç‚¹
        try:
            if hasattr(g, 'etypes') and len(g.etypes) > 1:
                # å¼‚æ„å›¾ä¸­æ·»åŠ è‡ªå¾ªç¯æ¯”è¾ƒéº»çƒ¦ï¼Œè·³è¿‡
                pass
            else:
                # åŒæ„å›¾ä¸­æ·»åŠ è‡ªå¾ªç¯
                g = dgl.add_self_loop(g)
                print("  å·²æ·»åŠ è‡ªå¾ªç¯")
        except Exception as e:
            print(f"  æ·»åŠ è‡ªå¾ªç¯å¤±è´¥: {e}")

        g.ndata['feat'] = torch.FloatTensor(node_features)
        g.ndata['label'] = torch.LongTensor(node_labels)
        g.ndata['train_mask'] = torch.BoolTensor(train_mask)
        g.ndata['valid_mask'] = torch.BoolTensor(valid_mask)
        g.ndata['test_mask'] = torch.BoolTensor(test_mask)
        g.ndata['labeled_mask'] = torch.BoolTensor(labeled_train_mask)

        # 7. ç»Ÿè®¡ä¿¡æ¯
        print("\nå›¾æ„å»ºç»Ÿè®¡ä¿¡æ¯:")
        print(f"  èŠ‚ç‚¹æ•°: {g.num_nodes()}")
        print(f"  è¾¹æ•°: {g.num_edges()}")
        print(f"  å…³ç³»ç±»å‹æ•°: {len(g.etypes) if hasattr(g, 'etypes') else 1}")
        print(f"  ç‰¹å¾ç»´åº¦: {node_feat_dim}")
        print(f"  è®­ç»ƒèŠ‚ç‚¹: {train_entity_count}")
        print(f"  æœ‰æ ‡ç­¾çš„è®­ç»ƒèŠ‚ç‚¹: {labeled_train_mask.sum()}")
        print(f"  éªŒè¯èŠ‚ç‚¹: {valid_entity_count}")
        print(f"  æµ‹è¯•èŠ‚ç‚¹: {test_entity_count}")

        # æ£€æŸ¥ç´¢å¼•é—®é¢˜
        labeled_indices = torch.where(g.ndata['labeled_mask'])[0]
        if len(labeled_indices) > 0:
            max_idx = labeled_indices.max().item()
            print(f"  æœ‰æ ‡ç­¾èŠ‚ç‚¹æœ€å¤§ç´¢å¼•: {max_idx}, èŠ‚ç‚¹æ€»æ•°: {g.num_nodes()}")
            if max_idx >= g.num_nodes():
                print(f"  è­¦å‘Š: ç´¢å¼•è¶Šç•Œ! max_idx={max_idx} >= num_nodes={g.num_nodes()}")

        # 8. ä¿å­˜å›¾æ•°æ®
        os.makedirs('processed_data', exist_ok=True)
        dgl.save_graphs('processed_data/fb15ket_graph.bin', [g])

        # ä¿å­˜æ˜ å°„å…³ç³»
        mapping_data = {
            'entity_id_map': entity_id_map,
            'relation_id_map': relation_id_map,
            'category_names': self.data_loader.category_names
        }
        torch.save(mapping_data, 'processed_data/fb15ket_mappings.pt')

        print(f"\nå›¾æ•°æ®å·²ä¿å­˜åˆ°: processed_data/fb15ket_graph.bin")
        print(f"æ˜ å°„æ•°æ®å·²ä¿å­˜åˆ°: processed_data/fb15ket_mappings.pt")

        return g, entity_id_map, relation_id_map


class FB15KETHetGNN(nn.Module):
    """é’ˆå¯¹FB15KETçš„å¼‚æ„å›¾ç¥ç»ç½‘ç»œ"""

    def __init__(self, in_feats, hid_feats, out_feats, num_relations):
        super().__init__()
        self.num_relations = num_relations

        # å…³ç³»åµŒå…¥
        self.relation_emb = nn.Embedding(num_relations, hid_feats)

        # ä¸ºæ¯ç§å…³ç³»ç±»å‹åˆ›å»ºå›¾å·ç§¯å±‚ï¼Œå…è®¸é›¶å…¥åº¦èŠ‚ç‚¹
        self.conv_layers = nn.ModuleDict()
        for i in range(num_relations):
            self.conv_layers[f'rel_{i}'] = dglnn.GraphConv(in_feats, hid_feats, allow_zero_in_degree=True)

        # èåˆå±‚
        self.fusion = nn.Linear(hid_feats * num_relations, out_feats)

        # BiLSTMç”¨äºä¿¡æ¯èåˆ
        self.bilstm = nn.LSTM(
            input_size=out_feats,
            hidden_size=out_feats // 2,
            num_layers=1,
            batch_first=True,
            bidirectional=True
        )

        # è‡ªå¾ªç¯å±‚ï¼Œç”¨äºå¤„ç†å­¤ç«‹èŠ‚ç‚¹
        self.self_loop_proj = nn.Linear(in_feats, out_feats)

    def forward(self, g, inputs):
        # inputs: [num_nodes, in_feats]
        h = inputs

        # å¯¹æ¯ç§å…³ç³»ç±»å‹åˆ†åˆ«è¿›è¡Œå·ç§¯
        relation_outputs = []
        for i, etype in enumerate(g.etypes):
            rel_emb = self.relation_emb(torch.tensor([i], device=inputs.device))

            # è·å–è¯¥å…³ç³»ç±»å‹çš„å­å›¾
            subgraph = g[etype]
            if subgraph.num_edges() > 0:
                try:
                    # åº”ç”¨å›¾å·ç§¯
                    conv_out = self.conv_layers[f'rel_{i}'](subgraph, h)
                    # åŠ å…¥å…³ç³»åµŒå…¥ä¿¡æ¯
                    conv_out = conv_out + rel_emb.expand_as(conv_out)
                    relation_outputs.append(conv_out)
                except Exception as e:
                    # å¦‚æœå·ç§¯å¤±è´¥ï¼Œä½¿ç”¨è¾“å…¥ç‰¹å¾
                    print(f"å…³ç³» {etype} å·ç§¯å¤±è´¥: {e}")
                    relation_outputs.append(h)
            else:
                # å¦‚æœæ²¡æœ‰è¾¹ï¼Œä½¿ç”¨è¾“å…¥ç‰¹å¾
                relation_outputs.append(h)

        if relation_outputs:
            # èåˆæ‰€æœ‰å…³ç³»ç±»å‹çš„è¾“å‡º
            combined = torch.cat(relation_outputs, dim=1)
            h = F.relu(self.fusion(combined))
        else:
            h = F.relu(self.fusion(h.repeat(1, self.num_relations)))

        # BiLSTMå¤„ç†
        if h.dim() == 2:
            h = h.unsqueeze(1)  # å¢åŠ batchç»´åº¦
            h, _ = self.bilstm(h)
            h = h.squeeze(1)

        return h


class FB15KETSubgraphBuilder:
    """é’ˆå¯¹FB15KETçš„å­å›¾æ„å»ºå™¨"""

    def __init__(self, hetero_graph):
        self.g = hetero_graph

    def get_relation_aware_neighbors(self, node_id, max_neighbors=50):
        """è·å–å…³ç³»æ„ŸçŸ¥çš„é‚»å±…"""
        neighbor_info = defaultdict(list)

        # æ£€æŸ¥èŠ‚ç‚¹æ˜¯å¦åœ¨å›¾ä¸­
        if node_id >= self.g.num_nodes():
            return neighbor_info

        # éå†æ‰€æœ‰å…³ç³»ç±»å‹
        for etype in self.g.etypes:
            try:
                # è·å–å‡ºè¾¹é‚»å±…
                successors = self.g.successors(node_id, etype=etype)
                if len(successors) > 0:
                    # é™åˆ¶é‚»å±…æ•°é‡
                    if len(successors) > max_neighbors:
                        indices = torch.randperm(len(successors))[:max_neighbors]
                        successors = successors[indices]

                    neighbor_info[etype].extend([(n.item(), etype, 'out') for n in successors])

                # è·å–å…¥è¾¹é‚»å±…
                predecessors = self.g.predecessors(node_id, etype=etype)
                if len(predecessors) > 0:
                    if len(predecessors) > max_neighbors:
                        indices = torch.randperm(len(predecessors))[:max_neighbors]
                        predecessors = predecessors[indices]

                    neighbor_info[etype].extend([(n.item(), etype, 'in') for n in predecessors])
            except Exception as e:
                # å¦‚æœè·å–é‚»å±…å¤±è´¥ï¼Œç»§ç»­ä¸‹ä¸€ä¸ªå…³ç³»ç±»å‹
                continue

        return neighbor_info

    def build_subgraph_embedding(self, node_id, node_feats, relation_embeddings=None):
        """æ„å»ºå­å›¾åµŒå…¥"""
        # æ£€æŸ¥èŠ‚ç‚¹IDæ˜¯å¦æœ‰æ•ˆ
        if node_id >= len(node_feats):
            # è¿”å›é›¶å‘é‡
            zero_emb = torch.zeros_like(node_feats[0])
            return zero_emb, {'self': 1.0, 'neighbor': 0.0, 'error': 'invalid_node_id'}

        # è·å–é‚»å±…ä¿¡æ¯
        neighbor_info = self.get_relation_aware_neighbors(node_id)

        if not neighbor_info:
            # æ— é‚»å±…ï¼Œè¿”å›èŠ‚ç‚¹è‡ªèº«ç‰¹å¾
            return node_feats[node_id], {'self': 1.0, 'neighbor': 0.0}

        # ä¸­å¿ƒèŠ‚ç‚¹ç‰¹å¾
        center_feat = node_feats[node_id]

        # èšåˆé‚»å±…ç‰¹å¾
        neighbor_embs = []
        relation_weights = []

        for etype, neighbors in neighbor_info.items():
            for neighbor_id, rel_type, direction in neighbors:
                # æ£€æŸ¥é‚»å±…IDæ˜¯å¦æœ‰æ•ˆ
                if neighbor_id >= len(node_feats):
                    continue

                neighbor_feat = node_feats[neighbor_id]

                # åŠ å…¥å…³ç³»ä¿¡æ¯
                if relation_embeddings is not None:
                    try:
                        rel_idx = int(rel_type.split('_')[1])  # ä»'rel_X'ä¸­æå–X
                        if rel_idx < len(relation_embeddings):
                            rel_emb = relation_embeddings[rel_idx]
                            combined = torch.cat([neighbor_feat, rel_emb])
                        else:
                            combined = neighbor_feat
                    except:
                        combined = neighbor_feat
                else:
                    combined = neighbor_feat

                neighbor_embs.append(combined)

                # å…³ç³»æƒé‡ï¼ˆå¯å­¦ä¹ æˆ–ç®€å•åˆ†é…ï¼‰
                if direction == 'out':
                    weight = 1.0  # å‡ºè¾¹
                else:
                    weight = 0.8  # å…¥è¾¹ï¼Œæƒé‡ç¨ä½

                relation_weights.append(weight)

        # åŠ æƒèšåˆ
        if neighbor_embs and len(neighbor_embs) > 0:
            weights = torch.softmax(torch.tensor(relation_weights), dim=0)
            weights = weights.to(node_feats.device)

            neighbor_tensor = torch.stack(neighbor_embs)
            aggregated = torch.sum(weights.unsqueeze(1) * neighbor_tensor, dim=0)

            # ä¸ä¸­å¿ƒèŠ‚ç‚¹ç‰¹å¾ç»“åˆ
            self_weight = 0.7
            neighbor_weight = 0.3

            final_emb = self_weight * center_feat + neighbor_weight * aggregated
        else:
            final_emb = center_feat
            self_weight = 1.0
            neighbor_weight = 0.0

        # æ„å»ºè§£é‡Šä¿¡æ¯
        components = {
            'self_contribution': self_weight,
            'neighbor_contribution': neighbor_weight,
            'neighbor_count': len(neighbor_embs)
        }

        return final_emb, components


class FB15KETPrototypeNetwork(nn.Module):
    """é’ˆå¯¹FB15KETçš„åŸå‹ç½‘ç»œ"""

    def __init__(self, feature_dim, num_classes, num_prototypes_per_class=3):
        super().__init__()
        self.feature_dim = feature_dim
        self.num_classes = num_classes
        self.num_prototypes = num_prototypes_per_class * num_classes

        # å¯å­¦ä¹ çš„åŸå‹
        self.prototypes = nn.Parameter(torch.randn(self.num_prototypes, feature_dim))

        # åŸå‹åˆ°ç±»åˆ«çš„æ˜ å°„
        self.prototype_to_class = torch.repeat_interleave(
            torch.arange(num_classes), num_prototypes_per_class
        )

        # æ¸©åº¦å‚æ•°
        self.temperature = nn.Parameter(torch.tensor(0.1))

        # åˆå§‹åŒ–åŸå‹
        self._init_prototypes()

    def _init_prototypes(self):
        """åˆå§‹åŒ–åŸå‹"""
        # ä½¿ç”¨Xavieråˆå§‹åŒ–
        nn.init.xavier_uniform_(self.prototypes)

    def forward(self, features):
        # æ£€æŸ¥ç‰¹å¾ç»´åº¦
        if features.size(1) != self.feature_dim:
            # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œå°è¯•è°ƒæ•´
            if features.size(1) > self.feature_dim:
                features = features[:, :self.feature_dim]
            else:
                # å¡«å……é›¶
                padding = torch.zeros(features.size(0), self.feature_dim - features.size(1),
                                      device=features.device)
                features = torch.cat([features, padding], dim=1)

        # è®¡ç®—ä¸æ‰€æœ‰åŸå‹çš„è·ç¦»
        distances = torch.cdist(features, self.prototypes, p=2)  # æ¬§æ°è·ç¦»

        # è½¬æ¢ä¸ºç›¸ä¼¼åº¦ï¼ˆè·ç¦»è¶Šå°ï¼Œç›¸ä¼¼åº¦è¶Šé«˜ï¼‰
        similarities = torch.exp(-distances / (self.temperature.abs() + 1e-8))

        # æŒ‰ç±»åˆ«èšåˆç›¸ä¼¼åº¦
        class_similarities = torch.zeros(features.size(0), self.num_classes, device=features.device)

        for c in range(self.num_classes):
            # è·å–è¯¥ç±»åˆ«çš„åŸå‹ç´¢å¼•
            proto_indices = (self.prototype_to_class == c).nonzero(as_tuple=True)[0]
            if len(proto_indices) > 0:
                # å–è¯¥ç±»åŸå‹ç›¸ä¼¼åº¦çš„æœ€å¤§å€¼
                if len(proto_indices) == 1:
                    class_similarities[:, c] = similarities[:, proto_indices[0]]
                else:
                    class_similarities[:, c] = similarities[:, proto_indices].max(dim=1).values

        # è®¡ç®—åˆ†ç±»æ¦‚ç‡
        probs = F.softmax(class_similarities, dim=1)

        return probs, similarities, class_similarities

    def update_prototypes(self, features, labels, learning_rate=0.01):
        """æ›´æ–°åŸå‹å‘é‡"""
        with torch.no_grad():
            for c in range(self.num_classes):
                # è·å–å±äºç±»åˆ«cçš„æ ·æœ¬
                mask = (labels == c)
                if mask.sum() > 0:
                    class_features = features[mask]

                    # è·å–è¯¥ç±»åˆ«çš„åŸå‹ç´¢å¼•
                    proto_indices = (self.prototype_to_class == c).nonzero(as_tuple=True)[0]

                    if len(class_features) > 0 and len(proto_indices) > 0:
                        # ä½¿ç”¨K-meansæ€æƒ³æ›´æ–°åŸå‹
                        centroids = []

                        # å¦‚æœæ ·æœ¬æ•°å°‘äºåŸå‹æ•°ï¼Œå¤åˆ¶æ ·æœ¬
                        if len(class_features) < len(proto_indices):
                            for i in range(len(proto_indices)):
                                idx = i % len(class_features)
                                centroids.append(class_features[idx])
                        else:
                            # ç®€å•èšç±»ï¼šå‡åŒ€é€‰æ‹©æ ·æœ¬
                            step = len(class_features) // len(proto_indices)
                            for i in range(len(proto_indices)):
                                idx = min(i * step, len(class_features) - 1)
                                centroids.append(class_features[idx])

                        if centroids:
                            new_prototypes = torch.stack(centroids)
                            # å¹³æ»‘æ›´æ–°
                            self.prototypes.data[proto_indices] = (
                                                                          1 - learning_rate) * self.prototypes.data[
                                                                      proto_indices] + learning_rate * new_prototypes


class FB15KETXGradNet(nn.Module):
    """å®Œæ•´çš„FB15KET XGradNetæ¨¡å‹"""

    def __init__(self, hetero_graph, feature_dim, hidden_dim=128, out_dim=64,
                 num_classes=9, num_prototypes_per_class=3):
        super().__init__()
        self.g = hetero_graph

        # æ£€æŸ¥å›¾æ˜¯å¦ä¸ºç©º
        if hetero_graph.num_edges() == 0:
            print("è­¦å‘Š: å›¾æ²¡æœ‰è¾¹ï¼Œæ¨¡å‹å¯èƒ½æ— æ³•å­¦ä¹ å…³ç³»ä¿¡æ¯")

        # 1. å¼‚æ„å›¾ç¥ç»ç½‘ç»œ
        num_relations = len(hetero_graph.etypes) if hasattr(hetero_graph, 'etypes') else 1
        self.hetgnn = FB15KETHetGNN(feature_dim, hidden_dim, out_dim, num_relations)

        # 2. å­å›¾æ„å»ºå™¨
        self.subgraph_builder = FB15KETSubgraphBuilder(hetero_graph)

        # 3. åŸå‹ç½‘ç»œ
        self.prototype_net = FB15KETPrototypeNetwork(
            out_dim, num_classes, num_prototypes_per_class
        )

        # 4. ç»“æ„è´¡çŒ®æƒé‡
        self.structure_weights = nn.Parameter(torch.tensor([0.8, 0.2]))  # [self, neighbor]

        # 5. ç‰¹å¾è½¬æ¢å±‚
        self.feature_transform = nn.Sequential(
            nn.Linear(feature_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, out_dim)
        )

        # 6. è¾“å‡ºå±‚ï¼ˆå¤‡ç”¨ï¼Œå¦‚æœåŸå‹ç½‘ç»œæ•ˆæœä¸å¥½ï¼‰
        self.fc = nn.Linear(out_dim, num_classes)

        # åˆå§‹åŒ–æƒé‡
        self._init_weights()

    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, node_features):
        """å‰å‘ä¼ æ’­"""
        # ç‰¹å¾è½¬æ¢
        transformed_features = self.feature_transform(node_features)

        # å¼‚æ„å›¾å·ç§¯
        try:
            node_embeddings = self.hetgnn(self.g, transformed_features)
        except Exception as e:
            print(f"å¼‚æ„å›¾å·ç§¯å¤±è´¥: {e}")
            # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨è½¬æ¢åçš„ç‰¹å¾
            node_embeddings = transformed_features

        return node_embeddings

    def classify(self, node_embeddings, node_ids, update_prototypes=False, labels=None):
        """åˆ†ç±»é¢„æµ‹"""
        if node_embeddings is None or len(node_ids) == 0:
            return None, None, None

        subgraph_embeddings = []
        components_list = []
        valid_node_ids = []

        # ä¸ºæ¯ä¸ªèŠ‚ç‚¹æ„å»ºå­å›¾åµŒå…¥
        for node_id in node_ids:
            # æ£€æŸ¥èŠ‚ç‚¹IDæ˜¯å¦æœ‰æ•ˆ
            if node_id < len(node_embeddings):
                # ä½¿ç”¨å½“å‰çš„ç»“æ„æƒé‡
                self_weight = torch.sigmoid(self.structure_weights[0])
                neighbor_weight = torch.sigmoid(self.structure_weights[1])

                subgraph_emb, components = self.subgraph_builder.build_subgraph_embedding(
                    node_id, node_embeddings
                )
                subgraph_embeddings.append(subgraph_emb)
                components_list.append(components)
                valid_node_ids.append(node_id)

        if not subgraph_embeddings:
            return None, None, None

        subgraph_embeddings = torch.stack(subgraph_embeddings)

        # åŸå‹ç½‘ç»œåˆ†ç±»
        probs, similarities, class_similarities = self.prototype_net(subgraph_embeddings)

        # æ›´æ–°åŸå‹
        if update_prototypes and labels is not None:
            # åªä½¿ç”¨æœ‰æ•ˆèŠ‚ç‚¹çš„æ ‡ç­¾
            valid_labels = labels[valid_node_ids]
            self.prototype_net.update_prototypes(subgraph_embeddings, valid_labels)

        # é¢„æµ‹ç±»åˆ«
        _, predicted_classes = torch.max(probs, dim=1)

        # æ„å»ºè§£é‡Šä¿¡æ¯
        explanations = {
            'similarities': similarities,
            'class_similarities': class_similarities,
            'components': components_list,
            'structure_weights': self.structure_weights,
            'prototypes': self.prototype_net.prototypes.data,
            'valid_node_ids': valid_node_ids
        }

        return probs, predicted_classes, explanations

    def simple_classify(self, node_embeddings, node_ids):
        """ç®€åŒ–åˆ†ç±»ï¼ˆä¸ä½¿ç”¨å­å›¾å’ŒåŸå‹ï¼‰"""
        if node_embeddings is None or len(node_ids) == 0:
            return None, None

        # ç›´æ¥ä½¿ç”¨å…¨è¿æ¥å±‚åˆ†ç±»
        embeddings = node_embeddings[node_ids]
        logits = self.fc(embeddings)
        probs = F.softmax(logits, dim=1)
        _, predicted_classes = torch.max(probs, dim=1)

        return probs, predicted_classes

    def get_interpretation(self, node_id, node_embeddings, class_names):
        """è·å–å•ä¸ªèŠ‚ç‚¹çš„è§£é‡Š"""
        probs, pred_class, explanations = self.classify(
            node_embeddings, [node_id], update_prototypes=False
        )

        if probs is None:
            # å¦‚æœåŸå‹åˆ†ç±»å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–åˆ†ç±»
            probs, pred_class = self.simple_classify(node_embeddings, [node_id])
            if probs is None:
                return None

            pred_class_idx = pred_class.item()
            pred_prob = probs[0, pred_class_idx].item()

            interpretation = {
                'predicted_class': {
                    'index': pred_class_idx,
                    'name': class_names.get(pred_class_idx + 1, f"ç±»åˆ«{pred_class_idx + 1}"),
                    'probability': pred_prob
                },
                'method': 'simple_classification'
            }
            return interpretation

        # æå–è§£é‡Šæ•°æ®
        pred_class_idx = pred_class.item()
        pred_prob = probs[0, pred_class_idx].item()

        # è·å–ä¸å„ç±»åˆ«çš„ç›¸ä¼¼åº¦
        class_sims = explanations['class_similarities'][0].cpu().detach().numpy()

        # è·å–ç»“æ„è´¡çŒ®
        components = explanations['components'][0]

        # æ„å»ºè§£é‡Šç»“æœ
        interpretation = {
            'predicted_class': {
                'index': pred_class_idx,
                'name': class_names.get(pred_class_idx + 1, f"ç±»åˆ«{pred_class_idx + 1}"),
                'probability': pred_prob
            },
            'class_similarities': {
                class_names.get(i + 1, f"ç±»åˆ«{i + 1}"): float(sim)
                for i, sim in enumerate(class_sims)
            },
            'structure_contributions': components,
            'top_prototype_similarities': explanations['similarities'][0].cpu().detach().numpy(),
            'method': 'prototype_classification'
        }

        return interpretation


class EnhancedFB15KETXGradNet(nn.Module):
    """å¢å¼ºçš„æ¨¡å‹æ¶æ„"""

    def __init__(self, feature_dim, hidden_dim=256, out_dim=128, num_classes=9,
                 num_prototypes_per_class=3, dropout_rate=0.5):
        super().__init__()

        # 1. æ›´æ·±çš„ç‰¹å¾è½¬æ¢ç½‘ç»œ
        self.feature_transform = nn.Sequential(
            nn.Linear(feature_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate),

            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.8),

            nn.Linear(hidden_dim // 2, out_dim),
            nn.BatchNorm1d(out_dim),
            nn.ReLU(),
            nn.Dropout(dropout_rate * 0.6)
        )

        # 2. æ³¨æ„åŠ›æœºåˆ¶
        self.attention = nn.MultiheadAttention(
            embed_dim=out_dim,
            num_heads=4,
            dropout=0.1,
            batch_first=True
        )

        # 3. åŸå‹ç½‘ç»œ
        self.prototype_net = FB15KETPrototypeNetwork(
            out_dim, num_classes, num_prototypes_per_class
        )

        # 4. åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(out_dim * 2, hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim // 2, num_classes)
        )

        # 5. æ®‹å·®è¿æ¥
        self.residual = nn.Linear(feature_dim, out_dim) if feature_dim != out_dim else nn.Identity()

        self._init_weights()

    def _init_weights(self):
        """æ›´å¥½çš„æƒé‡åˆå§‹åŒ–"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)

    def forward(self, node_features):
        """å‰å‘ä¼ æ’­"""
        # æ®‹å·®è¿æ¥
        residual = self.residual(node_features)

        # ç‰¹å¾è½¬æ¢
        transformed = self.feature_transform(node_features)

        # æ³¨æ„åŠ›æœºåˆ¶ï¼ˆå°†ç‰¹å¾è§†ä¸ºåºåˆ—ï¼‰
        batch_size = transformed.size(0)
        if batch_size > 1:
            # é‡å¡‘ä¸ºåºåˆ—å½¢å¼ [batch_size, 1, feature_dim]
            seq_features = transformed.unsqueeze(1)
            attn_output, _ = self.attention(seq_features, seq_features, seq_features)
            attn_features = attn_output.squeeze(1)
        else:
            attn_features = transformed

        # æ®‹å·®è¿æ¥
        combined = attn_features + residual

        return combined

    def classify(self, node_embeddings, node_ids, update_prototypes=False, labels=None):
        """åˆ†ç±»é¢„æµ‹"""
        if node_embeddings is None or len(node_ids) == 0:
            return None, None, None

        # è·å–æŒ‡å®šèŠ‚ç‚¹çš„åµŒå…¥
        embeddings = node_embeddings[node_ids]

        # åŸå‹ç½‘ç»œåˆ†ç±»
        prototype_probs, similarities, class_similarities = self.prototype_net(embeddings)

        # åˆ†ç±»å¤´åˆ†ç±»
        classifier_logits = self.classifier(
            torch.cat([embeddings, class_similarities], dim=1)
        )
        classifier_probs = F.softmax(classifier_logits, dim=1)

        # èåˆä¸¤ç§åˆ†ç±»ç»“æœ
        alpha = 0.7  # åŸå‹ç½‘ç»œæƒé‡
        combined_probs = alpha * prototype_probs + (1 - alpha) * classifier_probs

        # æ›´æ–°åŸå‹
        if update_prototypes and labels is not None:
            self.prototype_net.update_prototypes(embeddings, labels[node_ids])

        # é¢„æµ‹ç±»åˆ«
        _, predicted_classes = torch.max(combined_probs, dim=1)

        return combined_probs, predicted_classes, {
            'prototype_probs': prototype_probs,
            'classifier_probs': classifier_probs,
            'similarities': similarities,
            'class_similarities': class_similarities
        }

class SimpleFB15KETXGradNet(nn.Module):
    """ç®€åŒ–çš„FB15KET XGradNetæ¨¡å‹ï¼Œé¿å…å¼‚æ„å›¾çš„å¤æ‚æ“ä½œ"""

    def __init__(self, feature_dim, hidden_dim=128, out_dim=64, num_classes=9):
        super().__init__()

        # 1. ç‰¹å¾è½¬æ¢å±‚
        self.feature_transform = nn.Sequential(
            nn.Linear(feature_dim, hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),
            nn.Linear(hidden_dim, out_dim),
            nn.ReLU()
        )

        # 2. åŸå‹ç½‘ç»œ
        self.prototype_net = FB15KETPrototypeNetwork(
            out_dim, num_classes, num_prototypes_per_class=2
        )

        # 3. å¤‡ç”¨åˆ†ç±»å±‚
        self.fc = nn.Linear(out_dim, num_classes)

        # åˆå§‹åŒ–æƒé‡
        self._init_weights()

    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.zeros_(m.bias)

    def forward(self, node_features):
        """å‰å‘ä¼ æ’­ - ä»…ç‰¹å¾è½¬æ¢"""
        return self.feature_transform(node_features)

    def classify(self, node_embeddings, node_ids, update_prototypes=False, labels=None):
        """åˆ†ç±»é¢„æµ‹"""
        if node_embeddings is None or len(node_ids) == 0:
            return None, None, None

        # è·å–æŒ‡å®šèŠ‚ç‚¹çš„åµŒå…¥
        embeddings = node_embeddings[node_ids]

        # åŸå‹ç½‘ç»œåˆ†ç±»
        probs, similarities, class_similarities = self.prototype_net(embeddings)

        # æ›´æ–°åŸå‹
        if update_prototypes and labels is not None:
            self.prototype_net.update_prototypes(embeddings, labels[node_ids])

        # é¢„æµ‹ç±»åˆ«
        _, predicted_classes = torch.max(probs, dim=1)

        # æ„å»ºè§£é‡Šä¿¡æ¯
        explanations = {
            'similarities': similarities,
            'class_similarities': class_similarities,
            'prototypes': self.prototype_net.prototypes.data
        }

        return probs, predicted_classes, explanations

    def simple_classify(self, node_embeddings, node_ids):
        """ç®€åŒ–åˆ†ç±»ï¼ˆä¸ä½¿ç”¨åŸå‹ï¼‰"""
        if node_embeddings is None or len(node_ids) == 0:
            return None, None

        # ç›´æ¥ä½¿ç”¨å…¨è¿æ¥å±‚åˆ†ç±»
        embeddings = node_embeddings[node_ids]
        logits = self.fc(embeddings)
        probs = F.softmax(logits, dim=1)
        _, predicted_classes = torch.max(probs, dim=1)

        return probs, predicted_classes

    def get_interpretation(self, node_id, node_embeddings, class_names):
        """è·å–å•ä¸ªèŠ‚ç‚¹çš„è§£é‡Š"""
        # ç›´æ¥åˆ†ç±»
        probs, pred_class = self.simple_classify(node_embeddings, [node_id])
        if probs is None:
            return None

        pred_class_idx = pred_class.item()
        pred_prob = probs[0, pred_class_idx].item()

        # å°è¯•è·å–åŸå‹ç›¸ä¼¼åº¦
        try:
            embeddings = node_embeddings[[node_id]]
            _, similarities, class_similarities = self.prototype_net(embeddings)

            class_sims = class_similarities[0].cpu().detach().numpy()

            interpretation = {
                'predicted_class': {
                    'index': pred_class_idx,
                    'name': class_names.get(pred_class_idx + 1, f"ç±»åˆ«{pred_class_idx + 1}"),
                    'probability': pred_prob
                },
                'class_similarities': {
                    class_names.get(i + 1, f"ç±»åˆ«{i + 1}"): float(sim)
                    for i, sim in enumerate(class_sims)
                },
                'method': 'prototype_classification'
            }
        except:
            interpretation = {
                'predicted_class': {
                    'index': pred_class_idx,
                    'name': class_names.get(pred_class_idx + 1, f"ç±»åˆ«{pred_class_idx + 1}"),
                    'probability': pred_prob
                },
                'method': 'simple_classification'
            }

        return interpretation



class SemiSupervisedTrainer:
    """åŠç›‘ç£è®­ç»ƒå™¨"""

    def __init__(self, model, graph, device='cuda'):
        self.model = model
        self.g = graph
        self.device = device

        # å°†æ¨¡å‹å’Œå›¾ç§»åŠ¨åˆ°è®¾å¤‡
        self.model = self.model.to(device)

        try:
            self.g = self.g.to(device)
        except:
            print("è­¦å‘Š: æ— æ³•å°†å›¾ç§»åŠ¨åˆ°è®¾å¤‡ï¼Œå°†ä½¿ç”¨CPU")
            self.device = torch.device('cpu')
            self.model = self.model.to(self.device)

        # è·å–æ©ç 
        self.labeled_mask = self.g.ndata['labeled_mask'].to(self.device)
        self.train_mask = self.g.ndata['train_mask'].to(self.device)
        self.valid_mask = self.g.ndata['valid_mask'].to(self.device)
        self.test_mask = self.g.ndata['test_mask'].to(self.device)

        # æ ‡ç­¾
        self.labels = self.g.ndata['label'].to(self.device)

        # ç‰¹å¾
        self.features = self.g.ndata['feat'].to(self.device)

        # è·å–æœ‰æ ‡ç­¾çš„èŠ‚ç‚¹ç´¢å¼•
        self.labeled_indices = torch.where(self.labeled_mask)[0]

        print(f"è®­ç»ƒèŠ‚ç‚¹: {self.train_mask.sum().item()}")
        print(f"æœ‰æ ‡ç­¾çš„è®­ç»ƒèŠ‚ç‚¹: {len(self.labeled_indices)}")
        print(f"éªŒè¯èŠ‚ç‚¹: {self.valid_mask.sum().item()}")
        print(f"æµ‹è¯•èŠ‚ç‚¹: {self.test_mask.sum().item()}")

        # æ£€æŸ¥æ•°æ®
        if len(self.labeled_indices) == 0:
            print("è­¦å‘Š: æ²¡æœ‰æœ‰æ ‡ç­¾çš„è®­ç»ƒèŠ‚ç‚¹ï¼")

    def train(self, epochs=100, lr=0.001, weight_decay=1e-4,
              contrastive_weight=0.1, prototype_weight=0.05):
        """è®­ç»ƒæ¨¡å‹"""
        if len(self.labeled_indices) == 0:
            print("é”™è¯¯: æ²¡æœ‰æœ‰æ ‡ç­¾çš„è®­ç»ƒèŠ‚ç‚¹ï¼Œæ— æ³•è®­ç»ƒï¼")
            return [], []

        optimizer = torch.optim.Adam(
            self.model.parameters(), lr=lr, weight_decay=weight_decay
        )

        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
            optimizer, T_max=epochs, eta_min=1e-5
        )

        best_val_acc = 0
        best_epoch = 0
        patience = 30

        train_losses = []
        val_accuracies = []

        print("\nå¼€å§‹è®­ç»ƒ...")
        print("-" * 80)

        for epoch in range(epochs):
            self.model.train()

            try:
                # å‰å‘ä¼ æ’­
                node_embeddings = self.model(self.features)

                # åˆ†ç±»ï¼ˆåªå¯¹æœ‰æ ‡ç­¾çš„è®­ç»ƒèŠ‚ç‚¹ï¼‰
                probs, preds, explanations = self.model.classify(
                    node_embeddings, self.labeled_indices,
                    update_prototypes=True,
                    labels=self.labels[self.labeled_indices]
                )

                if probs is None:
                    # å¦‚æœåŸå‹åˆ†ç±»å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–åˆ†ç±»
                    print(f"Epoch {epoch + 1}: åŸå‹åˆ†ç±»å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–åˆ†ç±»")
                    probs, preds = self.model.simple_classify(
                        node_embeddings, self.labeled_indices
                    )

                    if probs is None:
                        print(f"Epoch {epoch + 1}: ç®€åŒ–åˆ†ç±»ä¹Ÿå¤±è´¥ï¼Œè·³è¿‡æœ¬è½®")
                        continue

                    # åªè®¡ç®—åˆ†ç±»æŸå¤±
                    cls_loss = F.cross_entropy(
                        probs, self.labels[self.labeled_indices]
                    )
                    total_loss = cls_loss

                else:
                    # è®¡ç®—æŸå¤±
                    # 1. åˆ†ç±»æŸå¤±
                    cls_loss = F.cross_entropy(
                        probs, self.labels[self.labeled_indices]
                    )

                    # 2. å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼ˆé¼“åŠ±ç›¸ä¼¼èŠ‚ç‚¹çš„åµŒå…¥æ¥è¿‘ï¼‰
                    contrastive_loss = self.compute_contrastive_loss(
                        node_embeddings, self.labeled_indices
                    )

                    # 3. åŸå‹å¤šæ ·æ€§æŸå¤±
                    prototype_loss = self.compute_prototype_diversity_loss()

                    # æ€»æŸå¤±
                    total_loss = cls_loss + contrastive_weight * contrastive_loss + prototype_weight * prototype_loss

                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                total_loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                optimizer.step()
                scheduler.step()

                # è®°å½•è®­ç»ƒæŸå¤±
                train_losses.append(total_loss.item())

            except Exception as e:
                print(f"Epoch {epoch + 1} è®­ç»ƒå‡ºé”™: {e}")
                # è·³è¿‡è¿™ä¸ªepoch
                continue

            # éªŒè¯
            if (epoch + 1) % 5 == 0 or epoch == epochs - 1:
                try:
                    val_acc = self.evaluate(mode='valid')
                    val_accuracies.append(val_acc)

                    print(f"Epoch {epoch + 1:3d}/{epochs} | "
                          f"Loss: {total_loss.item():.4f} | "
                          f"Val Acc: {val_acc:.4f} | "
                          f"LR: {scheduler.get_last_lr()[0]:.6f}")

                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
                    if val_acc > best_val_acc:
                        best_val_acc = val_acc
                        best_epoch = epoch

                        # ä¿å­˜æœ€ä½³æ¨¡å‹
                        torch.save({
                            'epoch': epoch,
                            'model_state_dict': self.model.state_dict(),
                            'optimizer_state_dict': optimizer.state_dict(),
                            'val_acc': val_acc,
                            'loss': total_loss.item()
                        }, 'models/best_model.pth')

                        print(f"  ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Acc: {val_acc:.4f})")

                except Exception as e:
                    print(f"Epoch {epoch + 1} éªŒè¯å‡ºé”™: {e}")
                    val_accuracies.append(0.0)

            ''''# æ—©åœ
            if epoch - best_epoch > patience:
                print(f"\næ—©åœåœ¨ epoch {epoch + 1}ï¼Œæœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
                break
            '''
        print("-" * 80)
        print(f"è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f} (epoch {best_epoch + 1})")

        # åŠ è½½æœ€ä½³æ¨¡å‹
        try:
            checkpoint = torch.load('models/best_model.pth', map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            print("å·²åŠ è½½æœ€ä½³æ¨¡å‹")
        except Exception as e:
            print(f"åŠ è½½æœ€ä½³æ¨¡å‹å¤±è´¥: {e}")

        return train_losses, val_accuracies

    def compute_contrastive_loss(self, embeddings, indices, temperature=0.1):
        """è®¡ç®—å¯¹æ¯”å­¦ä¹ æŸå¤±"""
        if len(indices) < 2:
            return torch.tensor(0.0, device=self.device)

        # é€‰æ‹©ä¸€éƒ¨åˆ†èŠ‚ç‚¹
        if len(indices) > 100:
            selected = indices[torch.randperm(len(indices))[:100]]
        else:
            selected = indices

        # è·å–åµŒå…¥
        selected_embeddings = embeddings[selected]

        # æ£€æŸ¥åµŒå…¥æ˜¯å¦æœ‰æ•ˆ
        if torch.isnan(selected_embeddings).any() or torch.isinf(selected_embeddings).any():
            return torch.tensor(0.0, device=self.device)

        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity = torch.matmul(selected_embeddings, selected_embeddings.T) / (temperature + 1e-8)

        # å¯¹è§’çº¿è®¾ç½®ä¸ºè´Ÿæ— ç©·ï¼ˆæ’é™¤è‡ªèº«ï¼‰
        mask = torch.eye(len(selected), device=self.device).bool()
        similarity.masked_fill_(mask, -1e9)

        # è®¡ç®—å¯¹æ¯”æŸå¤±
        labels = self.labels[selected]

        # åˆ›å»ºæ­£æ ·æœ¬å¯¹ï¼ˆç›¸åŒç±»åˆ«çš„èŠ‚ç‚¹ï¼‰
        label_matrix = labels.unsqueeze(0) == labels.unsqueeze(1)
        label_matrix.masked_fill_(mask, False)  # æ’é™¤è‡ªèº«

        # è®¡ç®—æŸå¤±
        pos_indices = label_matrix.nonzero()
        if len(pos_indices) == 0:
            pos_loss = torch.tensor(0.0, device=self.device)
        else:
            pos_similarity = similarity[label_matrix]
            pos_loss = -torch.mean(pos_similarity)

        # è´Ÿæ ·æœ¬æŸå¤±
        neg_indices = (~label_matrix).nonzero()
        if len(neg_indices) == 0:
            neg_loss = torch.tensor(0.0, device=self.device)
        else:
            neg_similarity = similarity[~label_matrix]
            neg_loss = torch.mean(torch.exp(neg_similarity))

        return pos_loss + torch.log(neg_loss + 1e-8)

    def compute_prototype_diversity_loss(self):
        """è®¡ç®—åŸå‹å¤šæ ·æ€§æŸå¤±"""
        try:
            prototypes = self.model.prototype_net.prototypes

            if prototypes.size(0) < 2:
                return torch.tensor(0.0, device=self.device)

            # è®¡ç®—åŸå‹é—´çš„è·ç¦»
            distances = torch.cdist(prototypes, prototypes, p=2)

            # æ’é™¤å¯¹è§’çº¿
            mask = torch.eye(prototypes.size(0), device=self.device).bool()
            distances = distances[~mask].view(prototypes.size(0), prototypes.size(0) - 1)

            # é¼“åŠ±åŸå‹é—´ä¿æŒä¸€å®šè·ç¦»
            min_distances = distances.min(dim=1).values
            diversity_loss = -torch.mean(min_distances)  # æœ€å°è·ç¦»è¶Šå¤§è¶Šå¥½

            return diversity_loss
        except Exception as e:
            print(f"è®¡ç®—åŸå‹å¤šæ ·æ€§æŸå¤±å¤±è´¥: {e}")
            return torch.tensor(0.0, device=self.device)

    def evaluate(self, mode='valid'):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()

        if mode == 'valid':
            mask = self.valid_mask
        elif mode == 'test':
            mask = self.test_mask
        elif mode == 'train':
            mask = self.labeled_mask
        else:
            raise ValueError(f"æœªçŸ¥è¯„ä¼°æ¨¡å¼: {mode}")

        indices = torch.where(mask)[0]

        if len(indices) == 0:
            return 0.0

        with torch.no_grad():
            try:
                node_embeddings = self.model(self.features)

                # å°è¯•ä½¿ç”¨åŸå‹åˆ†ç±»
                probs, preds, _ = self.model.classify(
                    node_embeddings, indices, update_prototypes=False
                )

                if probs is None:
                    # å¦‚æœåŸå‹åˆ†ç±»å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–åˆ†ç±»
                    probs, preds = self.model.simple_classify(node_embeddings, indices)

                    if probs is None:
                        return 0.0

                # è®¡ç®—å‡†ç¡®ç‡
                acc = (preds == self.labels[indices]).float().mean().item()

                return acc

            except Exception as e:
                print(f"è¯„ä¼°å¤±è´¥ ({mode}): {e}")
                return 0.0

    def test(self, save_results=True):
        """æµ‹è¯•æ¨¡å‹"""
        print("\n" + "=" * 60)
        print("æ¨¡å‹æµ‹è¯•")
        print("=" * 60)

        try:
            train_acc = self.evaluate(mode='train')
            valid_acc = self.evaluate(mode='valid')
            test_acc = self.evaluate(mode='test')

            print(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {train_acc:.4f}")
            print(f"éªŒè¯é›†å‡†ç¡®ç‡: {valid_acc:.4f}")
            print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f}")

            # ç”Ÿæˆè¯¦ç»†é¢„æµ‹ç»“æœ
            if save_results:
                self.save_predictions()

            return {
                'train_acc': train_acc,
                'valid_acc': valid_acc,
                'test_acc': test_acc
            }

        except Exception as e:
            print(f"æµ‹è¯•å¤±è´¥: {e}")
            return {
                'train_acc': 0.0,
                'valid_acc': 0.0,
                'test_acc': 0.0
            }

    def save_predictions(self):
        """ä¿å­˜é¢„æµ‹ç»“æœ"""
        self.model.eval()

        all_indices = torch.arange(self.g.num_nodes(), device=self.device)

        with torch.no_grad():
            try:
                node_embeddings = self.model(self.features)

                # å°è¯•ä½¿ç”¨åŸå‹åˆ†ç±»
                probs, preds, explanations = self.model.classify(
                    node_embeddings, all_indices, update_prototypes=False
                )

                if probs is None:
                    # å¦‚æœåŸå‹åˆ†ç±»å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–åˆ†ç±»
                    probs, preds = self.model.simple_classify(node_embeddings, all_indices)

                    if probs is None:
                        print("æ— æ³•ç”Ÿæˆé¢„æµ‹ç»“æœ")
                        return

                # è½¬æ¢é¢„æµ‹ç»“æœ
                predictions = preds.cpu().numpy() + 1  # è½¬æ¢å›1-9çš„ç±»åˆ«
                probabilities = probs.cpu().numpy()

                # åˆ›å»ºç»“æœDataFrame
                results = []
                for idx in range(len(all_indices)):
                    pred_class = predictions[idx]
                    true_class = self.labels[idx].item() + 1 if self.labels[idx] != -1 else -1

                    results.append({
                        'node_id': idx,
                        'predicted_class': pred_class,
                        'true_class': true_class,
                        'prediction_prob': probabilities[idx, pred_class - 1] if pred_class <= probabilities.shape[
                            1] else 0.0,
                        'in_train': self.train_mask[idx].item(),
                        'in_valid': self.valid_mask[idx].item(),
                        'in_test': self.test_mask[idx].item(),
                        'has_label': self.labels[idx] != -1
                    })

                results_df = pd.DataFrame(results)

                # ä¿å­˜ç»“æœ
                results_df.to_csv('predictions/fb15ket_predictions.csv', index=False)

                print(f"\né¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: predictions/fb15ket_predictions.csv")

                # ä¿å­˜ç±»åˆ«çº§åˆ«çš„æ€§èƒ½
                self.save_class_level_performance(predictions)

            except Exception as e:
                print(f"ä¿å­˜é¢„æµ‹ç»“æœå¤±è´¥: {e}")

    def save_class_level_performance(self, predictions):
        """ä¿å­˜ç±»åˆ«çº§åˆ«çš„æ€§èƒ½åˆ†æ"""
        try:
            from sklearn.metrics import classification_report, confusion_matrix

            # åªåˆ†ææœ‰çœŸå®æ ‡ç­¾çš„æµ‹è¯•é›†èŠ‚ç‚¹
            test_indices = torch.where(self.test_mask & (self.labels != -1))[0]

            if len(test_indices) == 0:
                print("æ²¡æœ‰æµ‹è¯•é›†æ ‡ç­¾ï¼Œæ— æ³•ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š")
                return

            y_true = self.labels[test_indices].cpu().numpy() + 1
            y_pred = predictions[test_indices.cpu().numpy()]

            # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
            category_names = FB15KETDataLoader().category_names
            class_names = [f"{i}: {name}" for i, name in category_names.items()]

            report = classification_report(
                y_true, y_pred,
                target_names=class_names,
                output_dict=True
            )

            # ä¿å­˜æŠ¥å‘Š
            report_df = pd.DataFrame(report).transpose()
            report_df.to_csv('predictions/classification_report.csv')

            print(f"åˆ†ç±»æŠ¥å‘Šå·²ä¿å­˜åˆ°: predictions/classification_report.csv")

            # æ··æ·†çŸ©é˜µ
            cm = confusion_matrix(y_true, y_pred)
            cm_df = pd.DataFrame(
                cm,
                index=class_names,
                columns=class_names
            )
            cm_df.to_csv('predictions/confusion_matrix.csv')

            print(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: predictions/confusion_matrix.csv")

        except Exception as e:
            print(f"ä¿å­˜ç±»åˆ«çº§åˆ«æ€§èƒ½å¤±è´¥: {e}")
class SimpleTrainer:
    """ç®€åŒ–è®­ç»ƒå™¨ï¼Œä¸ä¾èµ–å¤æ‚çš„å›¾ç»“æ„"""

    def __init__(self, model, graph, device='cuda'):
        self.model = model
        self.g = graph
        self.device = device

        # å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡
        self.model = self.model.to(device)

        # è·å–æ•°æ®
        self.features = self.g.ndata['feat'].to(device)
        self.labels = self.g.ndata['label'].to(device)

        # è·å–æ©ç 
        self.train_mask = self.g.ndata['train_mask'].to(device)
        self.valid_mask = self.g.ndata['valid_mask'].to(device)
        self.test_mask = self.g.ndata['test_mask'].to(device)
        self.labeled_mask = self.g.ndata['labeled_mask'].to(device)

        # è·å–æœ‰æ ‡ç­¾çš„èŠ‚ç‚¹ç´¢å¼•
        self.labeled_indices = torch.where(self.labeled_mask)[0]

        print(f"è®­ç»ƒèŠ‚ç‚¹: {self.train_mask.sum().item()}")
        print(f"æœ‰æ ‡ç­¾çš„è®­ç»ƒèŠ‚ç‚¹: {len(self.labeled_indices)}")
        print(f"éªŒè¯èŠ‚ç‚¹: {self.valid_mask.sum().item()}")
        print(f"æµ‹è¯•èŠ‚ç‚¹: {self.test_mask.sum().item()}")

        # æ£€æŸ¥æ•°æ®
        if len(self.labeled_indices) == 0:
            print("è­¦å‘Š: æ²¡æœ‰æœ‰æ ‡ç­¾çš„è®­ç»ƒèŠ‚ç‚¹ï¼")

    def train(self, epochs=50, lr=0.001, weight_decay=1e-4):
        """è®­ç»ƒæ¨¡å‹"""
        if len(self.labeled_indices) == 0:
            print("é”™è¯¯: æ²¡æœ‰æœ‰æ ‡ç­¾çš„è®­ç»ƒèŠ‚ç‚¹ï¼Œæ— æ³•è®­ç»ƒï¼")
            return [], []

        optimizer = torch.optim.Adam(
            self.model.parameters(), lr=lr, weight_decay=weight_decay
        )

        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)

        best_val_acc = 0
        best_epoch = 0
        patience = 20

        train_losses = []
        val_accuracies = []

        print("\nå¼€å§‹è®­ç»ƒ...")
        print("-" * 80)

        for epoch in range(epochs):
            self.model.train()

            try:
                # å‰å‘ä¼ æ’­
                node_embeddings = self.model(self.features)

                # åˆ†ç±»ï¼ˆåªå¯¹æœ‰æ ‡ç­¾çš„è®­ç»ƒèŠ‚ç‚¹ï¼‰
                probs, preds, explanations = self.model.classify(
                    node_embeddings, self.labeled_indices,
                    update_prototypes=True,
                    labels=self.labels[self.labeled_indices]
                )

                if probs is None:
                    # å¦‚æœåŸå‹åˆ†ç±»å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–åˆ†ç±»
                    probs, preds = self.model.simple_classify(
                        node_embeddings, self.labeled_indices
                    )

                    if probs is None:
                        print(f"Epoch {epoch + 1}: åˆ†ç±»å¤±è´¥ï¼Œè·³è¿‡æœ¬è½®")
                        continue

                # è®¡ç®—åˆ†ç±»æŸå¤±
                cls_loss = F.cross_entropy(
                    probs, self.labels[self.labeled_indices]
                )

                total_loss = cls_loss

                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                total_loss.backward()

                # æ¢¯åº¦è£å‰ª
                torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                optimizer.step()
                scheduler.step()

                # è®°å½•è®­ç»ƒæŸå¤±
                train_losses.append(total_loss.item())

            except Exception as e:
                print(f"Epoch {epoch + 1} è®­ç»ƒå‡ºé”™: {e}")
                continue

            # éªŒè¯
            if (epoch + 1) % 5 == 0 or epoch == epochs - 1:
                try:
                    val_acc = self.evaluate(mode='valid')
                    val_accuracies.append(val_acc)

                    print(f"Epoch {epoch + 1:3d}/{epochs} | "
                          f"Loss: {total_loss.item():.4f} | "
                          f"Val Acc: {val_acc:.4f} | "
                          f"LR: {scheduler.get_last_lr()[0]:.6f}")

                    # æ£€æŸ¥æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹
                    if val_acc > best_val_acc:
                        best_val_acc = val_acc
                        best_epoch = epoch

                        # ä¿å­˜æœ€ä½³æ¨¡å‹
                        torch.save({
                            'epoch': epoch,
                            'model_state_dict': self.model.state_dict(),
                            'optimizer_state_dict': optimizer.state_dict(),
                            'val_acc': val_acc,
                            'loss': total_loss.item()
                        }, 'models/best_model.pth')

                        print(f"  ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Acc: {val_acc:.4f})")

                except Exception as e:
                    print(f"Epoch {epoch + 1} éªŒè¯å‡ºé”™: {e}")
                    val_accuracies.append(0.0)
            '''
            # æ—©åœ
            if epoch - best_epoch > patience:
                print(f"\næ—©åœåœ¨ epoch {epoch + 1}ï¼Œæœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
                break
            '''
        print("-" * 80)
        print(f"è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f} (epoch {best_epoch + 1})")

        # åŠ è½½æœ€ä½³æ¨¡å‹
        try:
            checkpoint = torch.load('models/best_model.pth', map_location=self.device)
            self.model.load_state_dict(checkpoint['model_state_dict'])
            print("å·²åŠ è½½æœ€ä½³æ¨¡å‹")
        except Exception as e:
            print(f"åŠ è½½æœ€ä½³æ¨¡å‹å¤±è´¥: {e}")

        return train_losses, val_accuracies

    def evaluate(self, mode='valid'):
        """è¯„ä¼°æ¨¡å‹"""
        self.model.eval()

        if mode == 'valid':
            mask = self.valid_mask
        elif mode == 'test':
            mask = self.test_mask
        elif mode == 'train':
            mask = self.labeled_mask
        else:
            raise ValueError(f"æœªçŸ¥è¯„ä¼°æ¨¡å¼: {mode}")

        indices = torch.where(mask & (self.labels != -1))[0]  # åªè¯„ä¼°æœ‰æ ‡ç­¾çš„èŠ‚ç‚¹

        if len(indices) == 0:
            return 0.0

        with torch.no_grad():
            try:
                node_embeddings = self.model(self.features)

                # ä½¿ç”¨åŸå‹åˆ†ç±»
                probs, preds, _ = self.model.classify(
                    node_embeddings, indices, update_prototypes=False
                )

                if probs is None:
                    # å¦‚æœåŸå‹åˆ†ç±»å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–åˆ†ç±»
                    probs, preds = self.model.simple_classify(node_embeddings, indices)

                    if probs is None:
                        return 0.0

                # è®¡ç®—å‡†ç¡®ç‡
                acc = (preds == self.labels[indices]).float().mean().item()

                return acc

            except Exception as e:
                print(f"è¯„ä¼°å¤±è´¥ ({mode}): {e}")
                return 0.0

    def test(self, save_results=True):
        """æµ‹è¯•æ¨¡å‹"""
        print("\n" + "=" * 60)
        print("æ¨¡å‹æµ‹è¯•")
        print("=" * 60)

        try:
            train_acc = self.evaluate(mode='train')
            valid_acc = self.evaluate(mode='valid')
            test_acc = self.evaluate(mode='test')

            print(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {train_acc:.4f}")
            print(f"éªŒè¯é›†å‡†ç¡®ç‡: {valid_acc:.4f}")
            print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f}")

            # ç”Ÿæˆè¯¦ç»†é¢„æµ‹ç»“æœ
            if save_results:
                self.save_predictions()

            return {
                'train_acc': train_acc,
                'valid_acc': valid_acc,
                'test_acc': test_acc
            }

        except Exception as e:
            print(f"æµ‹è¯•å¤±è´¥: {e}")
            return {
                'train_acc': 0.0,
                'valid_acc': 0.0,
                'test_acc': 0.0
            }

    def save_predictions(self):
        """ä¿å­˜é¢„æµ‹ç»“æœï¼ˆä¿®å¤numpyé—®é¢˜ï¼‰"""
        self.model.eval()

        all_indices = torch.arange(self.g.num_nodes(), device=self.device)
        labeled_indices = all_indices[self.labels != -1]

        with torch.no_grad():
            try:
                node_embeddings = self.model(self.features)

                # é¢„æµ‹æ‰€æœ‰æœ‰æ ‡ç­¾çš„èŠ‚ç‚¹
                probs, preds, _ = self.model.classify(
                    node_embeddings, labeled_indices, update_prototypes=False
                )

                if probs is None:
                    # å¦‚æœåŸå‹åˆ†ç±»å¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–åˆ†ç±»
                    probs, preds = self.model.simple_classify(node_embeddings, labeled_indices)

                    if probs is None:
                        print("æ— æ³•ç”Ÿæˆé¢„æµ‹ç»“æœ")
                        return

                # è½¬æ¢é¢„æµ‹ç»“æœ - ç¡®ä¿åœ¨CPUä¸Šæ“ä½œ
                predictions = preds.cpu().numpy() + 1  # è½¬æ¢å›1-9çš„ç±»åˆ«
                probabilities = probs.cpu().numpy()

                # åˆ›å»ºç»“æœDataFrame
                results = []
                for i, idx in enumerate(labeled_indices.cpu().numpy()):
                    pred_class = predictions[i]
                    true_class = self.labels[idx].item() + 1

                    results.append({
                        'node_id': int(idx),
                        'predicted_class': int(pred_class),
                        'true_class': int(true_class),
                        'prediction_prob': float(
                            probabilities[i, pred_class - 1] if pred_class <= probabilities.shape[1] else 0.0),
                        'in_train': bool(self.train_mask[idx].item()),
                        'in_valid': bool(self.valid_mask[idx].item()),
                        'in_test': bool(self.test_mask[idx].item())
                    })

                results_df = pd.DataFrame(results)

                # ä¿å­˜ç»“æœ
                os.makedirs('predictions', exist_ok=True)
                results_df.to_csv('predictions/fb15ket_predictions.csv', index=False)

                print(f"\né¢„æµ‹ç»“æœå·²ä¿å­˜åˆ°: predictions/fb15ket_predictions.csv")

                # ä¿å­˜ç±»åˆ«çº§åˆ«çš„æ€§èƒ½
                self.save_class_level_performance(predictions, labeled_indices.cpu().numpy())

            except Exception as e:
                print(f"ä¿å­˜é¢„æµ‹ç»“æœå¤±è´¥: {e}")
                import traceback
                traceback.print_exc()

    def save_class_level_performance(self, predictions, labeled_indices):
        """ä¿å­˜ç±»åˆ«çº§åˆ«çš„æ€§èƒ½åˆ†æ"""
        try:
            from sklearn.metrics import classification_report, confusion_matrix

            # è·å–æµ‹è¯•é›†çš„é¢„æµ‹ç»“æœ
            test_indices = []
            test_preds = []
            test_labels = []

            for i, idx in enumerate(labeled_indices):
                if self.test_mask[idx] and self.labels[idx] != -1:
                    test_indices.append(i)
                    test_preds.append(predictions[i])
                    test_labels.append(self.labels[idx].item() + 1)

            if len(test_indices) == 0:
                print("æ²¡æœ‰æµ‹è¯•é›†æ ‡ç­¾ï¼Œæ— æ³•ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š")
                return

            y_true = np.array(test_labels)
            y_pred = np.array(test_preds)

            # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
            category_names = FB15KETDataLoader().category_names
            class_names = [f"{i}: {name[:15]}..." for i, name in category_names.items()]

            report = classification_report(
                y_true, y_pred,
                target_names=class_names,
                output_dict=True
            )

            # ä¿å­˜æŠ¥å‘Š
            report_df = pd.DataFrame(report).transpose()
            report_df.to_csv('predictions/classification_report.csv')

            print(f"åˆ†ç±»æŠ¥å‘Šå·²ä¿å­˜åˆ°: predictions/classification_report.csv")

            # æ··æ·†çŸ©é˜µ
            cm = confusion_matrix(y_true, y_pred)
            cm_df = pd.DataFrame(
                cm,
                index=class_names,
                columns=class_names
            )
            cm_df.to_csv('predictions/confusion_matrix.csv')

            print(f"æ··æ·†çŸ©é˜µå·²ä¿å­˜åˆ°: predictions/confusion_matrix.csv")

        except Exception as e:
            print(f"ä¿å­˜ç±»åˆ«çº§åˆ«æ€§èƒ½å¤±è´¥: {e}")
class EnhancedTrainer(SimpleTrainer):
    """å¢å¼ºçš„è®­ç»ƒå™¨"""

    def train(self, epochs=100, lr=0.001, weight_decay=1e-4,
              warmup_epochs=10, patience=30):
        """æ”¹è¿›çš„è®­ç»ƒç­–ç•¥"""

        optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=lr,
            weight_decay=weight_decay,
            betas=(0.9, 0.999)
        )

        # å¸¦warmupçš„å­¦ä¹ ç‡è°ƒåº¦
        def lr_lambda(epoch):
            if epoch < warmup_epochs:
                # warmupé˜¶æ®µï¼šçº¿æ€§å¢åŠ å­¦ä¹ ç‡
                return (epoch + 1) / warmup_epochs
            else:
                # cosineè¡°å‡
                progress = (epoch - warmup_epochs) / (epochs - warmup_epochs)
                return 0.5 * (1 + math.cos(math.pi * progress))

        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)

        # æ··åˆç²¾åº¦è®­ç»ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
        scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None

        best_val_acc = 0
        best_epoch = 0

        train_losses = []
        val_accuracies = []

        print("\nå¼€å§‹å¢å¼ºè®­ç»ƒ...")
        print("-" * 80)

        for epoch in range(epochs):
            self.model.train()

            try:
                # æ··åˆç²¾åº¦è®­ç»ƒ
                if scaler is not None:
                    with torch.cuda.amp.autocast():
                        # å‰å‘ä¼ æ’­
                        node_embeddings = self.model(self.features)

                        # åˆ†ç±»
                        probs, preds, explanations = self.model.classify(
                            node_embeddings, self.labeled_indices,
                            update_prototypes=True,
                            labels=self.labels[self.labeled_indices]
                        )

                        if probs is None:
                            continue

                        # è®¡ç®—æŸå¤±
                        cls_loss = F.cross_entropy(
                            probs, self.labels[self.labeled_indices]
                        )

                        # æ·»åŠ æ­£åˆ™åŒ–æŸå¤±
                        reg_loss = 0.0
                        for param in self.model.parameters():
                            if param.requires_grad:
                                reg_loss += torch.norm(param, p=2)

                        total_loss = cls_loss + 1e-4 * reg_loss

                    # åå‘ä¼ æ’­
                    optimizer.zero_grad()
                    scaler.scale(total_loss).backward()

                    # æ¢¯åº¦è£å‰ª
                    scaler.unscale_(optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                    scaler.step(optimizer)
                    scaler.update()
                else:
                    # æ™®é€šè®­ç»ƒ
                    node_embeddings = self.model(self.features)

                    # åˆ†ç±»
                    probs, preds, explanations = self.model.classify(
                        node_embeddings, self.labeled_indices,
                        update_prototypes=True,
                        labels=self.labels[self.labeled_indices]
                    )

                    if probs is None:
                        continue

                    # è®¡ç®—æŸå¤±
                    cls_loss = F.cross_entropy(
                        probs, self.labels[self.labeled_indices]
                    )

                    # æ·»åŠ æ ‡ç­¾å¹³æ»‘
                    smooth_labels = self.label_smoothing(
                        self.labels[self.labeled_indices],
                        num_classes=9,
                        smoothing=0.1
                    )
                    smooth_loss = F.kl_div(
                        F.log_softmax(probs, dim=1),
                        smooth_labels,
                        reduction='batchmean'
                    )

                    total_loss = 0.7 * cls_loss + 0.3 * smooth_loss

                    # åå‘ä¼ æ’­
                    optimizer.zero_grad()
                    total_loss.backward()

                    # æ¢¯åº¦è£å‰ª
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                    optimizer.step()

                # æ›´æ–°å­¦ä¹ ç‡
                scheduler.step()

                # è®°å½•è®­ç»ƒæŸå¤±
                train_losses.append(total_loss.item())

            except Exception as e:
                print(f"Epoch {epoch + 1} è®­ç»ƒå‡ºé”™: {e}")
                continue

            # éªŒè¯
            if (epoch + 1) % 3 == 0 or epoch == epochs - 1:
                try:
                    val_acc = self.evaluate(mode='valid')
                    val_accuracies.append(val_acc)

                    current_lr = scheduler.get_last_lr()[0]
                    print(f"Epoch {epoch + 1:3d}/{epochs} | "
                          f"Loss: {total_loss.item():.4f} | "
                          f"Val Acc: {val_acc:.4f} | "
                          f"LR: {current_lr:.6f}")

                    # ä¿å­˜æœ€ä½³æ¨¡å‹
                    if val_acc > best_val_acc:
                        best_val_acc = val_acc
                        best_epoch = epoch

                        torch.save({
                            'epoch': epoch,
                            'model_state_dict': self.model.state_dict(),
                            'optimizer_state_dict': optimizer.state_dict(),
                            'scheduler_state_dict': scheduler.state_dict(),
                            'val_acc': val_acc,
                            'loss': total_loss.item(),
                            'config': {
                                'hidden_dim': self.model.feature_transform[0].out_features,
                                'dropout_rate': 0.5
                            }
                        }, 'best_enhanced_model.pth')

                        print(f"  ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹ (Val Acc: {val_acc:.4f})")

                except Exception as e:
                    print(f"Epoch {epoch + 1} éªŒè¯å‡ºé”™: {e}")
                    val_accuracies.append(0.0)

            # æ—©åœ
            if epoch - best_epoch > patience:
                print(f"\nâ¹ï¸  æ—©åœåœ¨ epoch {epoch + 1}ï¼Œæœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
                break

        print("-" * 80)
        print(f"âœ… è®­ç»ƒå®Œæˆï¼Œæœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f} (epoch {best_epoch + 1})")

        return train_losses, val_accuracies

    def label_smoothing(self, labels, num_classes, smoothing=0.1):
        """æ ‡ç­¾å¹³æ»‘"""
        confidence = 1.0 - smoothing
        smooth_labels = torch.full((labels.size(0), num_classes), smoothing / (num_classes - 1))
        smooth_labels.scatter_(1, labels.unsqueeze(1), confidence)
        return smooth_labels

def main():
    """ä¸»å‡½æ•°ï¼šå®Œæ•´çš„è®­ç»ƒè¯„ä¼°æµç¨‹ï¼ˆæœ€ç»ˆç‰ˆæœ¬ï¼‰"""
    print("=" * 80)
    print("åŸºäºåŸå‹çš„å¯è§£é‡Šæ€§FB15KETå®ä½“åˆ†ç±»ç³»ç»Ÿ")
    print("=" * 80)

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)

    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs('predictions', exist_ok=True)
    os.makedirs('models', exist_ok=True)
    os.makedirs('visualizations', exist_ok=True)
    os.makedirs('processed_data', exist_ok=True)

    try:
        # 1. æ£€æŸ¥æ•°æ®è´¨é‡
        print("\n[æ­¥éª¤1] æ•°æ®è´¨é‡æ£€æŸ¥...")
        data_loader = FB15KETDataLoader()
        stats = data_loader.analyze_data_quality()

        # 2. æ„å»ºå›¾
        print("\n[æ­¥éª¤2] æ„å»ºå¼‚æ„å›¾...")
        graph_builder = FB15KETGraphBuilder()

        # ç®€åŒ–ï¼šä½¿ç”¨åŒæ„å›¾ï¼Œé¿å…å¼‚æ„å›¾çš„å¤æ‚æ“ä½œ
        print("æ³¨æ„ï¼šä½¿ç”¨åŒæ„å›¾ç®€åŒ–å¤„ç†")
        g, entity_map, relation_map = graph_builder.build_heterogeneous_graph(
            use_relation_types=False  # ä¸ä½¿ç”¨å¼‚æ„è¾¹
        )

        if g is None:
            print("å›¾æ„å»ºå¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
            return

        # 3. åˆ›å»ºæ¨¡å‹
        print("\n[æ­¥éª¤3] åˆ›å»ºæ¨¡å‹...")
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")

        feature_dim = g.ndata['feat'].shape[1]
        hidden_dim = 128
        out_dim = 64
        num_classes = 9

        # ä½¿ç”¨ç®€åŒ–æ¨¡å‹
        model = SimpleFB15KETXGradNet(
            feature_dim, hidden_dim, out_dim, num_classes
        )

        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"æ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

        # 4. è®­ç»ƒæ¨¡å‹
        print("\n[æ­¥éª¤4] è®­ç»ƒæ¨¡å‹...")
        trainer = SimpleTrainer(model, g, device=device)

        # è®­ç»ƒ
        train_losses, val_accuracies = trainer.train(
            epochs=50,
            lr=0.001,
            weight_decay=1e-4
        )

        # 5. è¯„ä¼°æ¨¡å‹
        print("\n[æ­¥éª¤5] è¯„ä¼°æ¨¡å‹...")
        results = trainer.test(save_results=True)

        # 6. ç”Ÿæˆå¯è§£é‡Šæ€§åˆ†æ
        print("\n[æ­¥éª¤6] ç”Ÿæˆå¯è§£é‡Šæ€§åˆ†æ...")
        try:
            # åŠ è½½æ˜ å°„æ•°æ®
            mapping_data = torch.load('processed_data/fb15ket_mappings.pt')
            category_names = mapping_data['category_names']

            # é€‰æ‹©ä¸€äº›ç¤ºä¾‹èŠ‚ç‚¹è¿›è¡Œåˆ†æ
            test_indices = torch.where(g.ndata['test_mask'] & (g.ndata['label'] != -1))[0]

            if len(test_indices) > 0:
                # éšæœºé€‰æ‹©5ä¸ªèŠ‚ç‚¹
                if len(test_indices) > 5:
                    sample_indices = test_indices[torch.randperm(len(test_indices))[:5]]
                else:
                    sample_indices = test_indices

                interpretations = []
                for idx in sample_indices:
                    interpretation = model.get_interpretation(
                        idx.item(), model(g.ndata['feat'].to(device)), category_names
                    )

                    if interpretation is not None:
                        interpretations.append({
                            'node_id': idx.item(),
                            'interpretation': interpretation
                        })

                # ä¿å­˜è§£é‡Šç»“æœ
                if interpretations:
                    import json

                    # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
                    serializable_interpretations = []
                    for item in interpretations:
                        node_id = item['node_id']
                        interp = item['interpretation']

                        serializable = {
                            'node_id': node_id,
                            'predicted_class': interp['predicted_class'],
                            'method': interp.get('method', 'unknown')
                        }

                        if 'class_similarities' in interp:
                            serializable['class_similarities'] = interp['class_similarities']

                        serializable_interpretations.append(serializable)

                    # ä¿å­˜ä¸ºJSON
                    with open('predictions/interpretations.json', 'w', encoding='utf-8') as f:
                        json.dump(serializable_interpretations, f, ensure_ascii=False, indent=2)

                    print(f"å¯è§£é‡Šæ€§åˆ†æå·²ä¿å­˜åˆ°: predictions/interpretations.json")

                    # æ‰“å°ä¸€ä¸ªç¤ºä¾‹
                    print("\nç¤ºä¾‹è§£é‡Šåˆ†æ:")
                    print("-" * 60)
                    example = serializable_interpretations[0]
                    print(f"èŠ‚ç‚¹ID: {example['node_id']}")
                    print(f"é¢„æµ‹ç±»åˆ«: {example['predicted_class']['name']} "
                          f"(æ¦‚ç‡: {example['predicted_class']['probability']:.3f})")

                    if 'class_similarities' in example:
                        print("\nä¸å„ç±»åˆ«çš„ç›¸ä¼¼åº¦:")
                        for class_name, similarity in example['class_similarities'].items():
                            print(f"  {class_name}: {similarity:.4f}")

        except Exception as e:
            print(f"ç”Ÿæˆå¯è§£é‡Šæ€§åˆ†æå¤±è´¥: {e}")

        # 7. ç”Ÿæˆå¯è§†åŒ–
        print("\n[æ­¥éª¤7] ç”Ÿæˆå¯è§†åŒ–...")
        try:
            from visualization import VisualizationTool
            predictions_df = pd.read_csv('predictions/fb15ket_predictions.csv')
            viz_tool = VisualizationTool(model, g, category_names)
            viz_tool.plot_class_distribution(predictions_df)

            # ç”ŸæˆæŠ¥å‘Š
            results = {
                'train_acc': predictions_df[predictions_df['in_train']]
                .apply(lambda row: row['predicted_class'] == row['true_class'], axis=1).mean(),
                'valid_acc': predictions_df[predictions_df['in_valid']]
                .apply(lambda row: row['predicted_class'] == row['true_class'], axis=1).mean(),
                'test_acc': predictions_df[predictions_df['in_test']]
                .apply(lambda row: row['predicted_class'] == row['true_class'], axis=1).mean()
            }

            viz_tool.generate_report(results, predictions_df)

        except Exception as e:
            print(f"ç”Ÿæˆå¯è§†åŒ–å¤±è´¥: {e}")

        print("\n" + "=" * 80)
        print("æ‰€æœ‰æ­¥éª¤å®Œæˆï¼")
        print("=" * 80)

        return model, g, results

    except Exception as e:
        print(f"ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None


def fix_137_dimension_issue():
    """ä¸“é—¨ä¿®å¤137ç»´ç‰¹å¾é—®é¢˜"""
    print("=" * 80)
    print("FB15KETå®ä½“åˆ†ç±»ç³»ç»Ÿ - 137ç»´ç‰¹å¾ä¿®å¤ç‰ˆ")
    print("=" * 80)

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)

    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs('predictions', exist_ok=True)
    os.makedirs('models', exist_ok=True)

    try:
        # 1. åŠ è½½å›¾æ•°æ®
        print("\n[1] åŠ è½½å›¾æ•°æ®...")
        g_list, _ = dgl.load_graphs('processed_data/fb15ket_graph.bin')
        g = g_list[0]

        # æ£€æŸ¥ç‰¹å¾ç»´åº¦
        feature_dim = g.ndata['feat'].shape[1]
        print(f"æ£€æµ‹åˆ°çš„ç‰¹å¾ç»´åº¦: {feature_dim}")

        # 2. åˆ›å»ºä¸“é—¨å¤„ç†137ç»´çš„æ¨¡å‹
        print("\n[2] åˆ›å»º137ç»´ä¸“ç”¨æ¨¡å‹...")
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

        # ä¸“é—¨ä¸º137ç»´ç‰¹å¾è®¾è®¡çš„æ¨¡å‹
        class Model137D(nn.Module):
            def __init__(self, input_dim=137, hidden_dim=256, out_dim=128, num_classes=9):
                super().__init__()

                print(f"åˆ›å»º137ç»´ä¸“ç”¨æ¨¡å‹: {input_dim} -> {hidden_dim} -> {out_dim}")

                # è¾“å…¥å±‚ï¼ˆ137ç»´ä¸“ç”¨ï¼‰
                self.input_layer = nn.Sequential(
                    nn.Linear(input_dim, 256),
                    nn.BatchNorm1d(256),
                    nn.ReLU(),
                    nn.Dropout(0.3)
                )

                # éšè—å±‚
                self.hidden_layers = nn.Sequential(
                    nn.Linear(256, 128),
                    nn.BatchNorm1d(128),
                    nn.ReLU(),
                    nn.Dropout(0.2),

                    nn.Linear(128, 64),
                    nn.BatchNorm1d(64),
                    nn.ReLU(),
                    nn.Dropout(0.1)
                )

                # è¾“å‡ºå±‚
                self.classifier = nn.Linear(64, num_classes)

                # åˆå§‹åŒ–
                self._init_weights()

            def _init_weights(self):
                for m in self.modules():
                    if isinstance(m, nn.Linear):
                        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                        if m.bias is not None:
                            nn.init.zeros_(m.bias)
                    elif isinstance(m, nn.BatchNorm1d):
                        nn.init.ones_(m.weight)
                        nn.init.zeros_(m.bias)

            def forward(self, x):
                x = self.input_layer(x)
                x = self.hidden_layers(x)
                return x

            def classify(self, embeddings, indices, update_prototypes=False, labels=None):
                if embeddings is None or len(indices) == 0:
                    return None, None, None

                node_embeddings = embeddings[indices]
                logits = self.classifier(node_embeddings)
                probs = F.softmax(logits, dim=1)
                _, predicted_classes = torch.max(probs, dim=1)

                return probs, predicted_classes, {}

            def simple_classify(self, embeddings, indices):
                return self.classify(embeddings, indices)

        model = Model137D(input_dim=feature_dim)
        model = model.to(device)

        # 3. åˆ›å»ºè®­ç»ƒå™¨
        print("\n[3] åˆ›å»ºè®­ç»ƒå™¨...")
        trainer = SimpleTrainer(model, g, device=device)

        # 4. æµ‹è¯•å‰å‘ä¼ æ’­
        print("\n[4] æµ‹è¯•å‰å‘ä¼ æ’­...")
        test_features = g.ndata['feat'].to(device)
        test_output = model(test_features)
        print(f"âœ“ å‰å‘ä¼ æ’­æµ‹è¯•æˆåŠŸ")
        print(f"  è¾“å…¥: {test_features.shape}")
        print(f"  è¾“å‡º: {test_output.shape}")

        # 5. è®­ç»ƒæ¨¡å‹
        print("\n[5] è®­ç»ƒæ¨¡å‹...")
        train_losses, val_accuracies = trainer.train(
            epochs=50,  # å…ˆè®­ç»ƒ50ä¸ªepoch
            lr=0.001,
            weight_decay=1e-4
        )

        # 6. è¯„ä¼°æ¨¡å‹
        print("\n[6] è¯„ä¼°æ¨¡å‹...")
        results = trainer.test(save_results=True)

        # 7. ä¿å­˜æ¨¡å‹
        print("\n[7] ä¿å­˜æ¨¡å‹...")
        torch.save({
            'model_state_dict': model.state_dict(),
            'feature_dim': feature_dim,
            'results': results
        }, 'models/model_137d.pth')

        print(f"\næ¨¡å‹å·²ä¿å­˜åˆ°: models/model_137d.pth")

        return model, g, results

    except Exception as e:
        print(f"ä¿®å¤è¿‡ç¨‹å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None
def improved_main():
    """æ”¹è¿›çš„ä¸»å‡½æ•°ï¼ˆä¿®å¤ç»´åº¦é—®é¢˜ï¼‰"""
    print("=" * 80)
    print("FB15KETå®ä½“åˆ†ç±»ç³»ç»Ÿ - æ”¹è¿›ç‰ˆæœ¬")
    print("=" * 80)

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(42)

    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs('predictions', exist_ok=True)
    os.makedirs('models', exist_ok=True)
    os.makedirs('visualizations', exist_ok=True)
    os.makedirs('processed_data', exist_ok=True)
    os.makedirs('enhanced_data', exist_ok=True)

    try:
        # ============================================
        # 1. æ•°æ®è´¨é‡æ£€æŸ¥
        # ============================================
        print("\n[1/7] æ•°æ®è´¨é‡æ£€æŸ¥...")
        data_loader = FB15KETDataLoader()
        stats = data_loader.analyze_data_quality()

        # ============================================
        # 2. åŠ è½½æˆ–æ„å»ºå›¾æ•°æ®
        # ============================================
        print("\n[2/7] åŠ è½½å›¾æ•°æ®...")

        # é¦–å…ˆæ£€æŸ¥æ˜¯å¦æœ‰åŸå§‹çš„å›¾æ•°æ®
        if os.path.exists('processed_data/fb15ket_graph.bin'):
            print("åŠ è½½åŸå§‹å›¾æ•°æ®...")
            g_list, _ = dgl.load_graphs('processed_data/fb15ket_graph.bin')
            g = g_list[0]

            # è·å–ç‰¹å¾ç»´åº¦
            feature_dim = g.ndata['feat'].shape[1]
            print(f"åŸå§‹ç‰¹å¾ç»´åº¦: {feature_dim}")

            # æ£€æŸ¥æ•°æ®é›†åˆ’åˆ†é—®é¢˜
            print("\næ£€æŸ¥æ•°æ®é›†åˆ’åˆ†...")
            train_mask = g.ndata['train_mask'].sum().item()
            valid_mask = g.ndata['valid_mask'].sum().item()
            test_mask = g.ndata['test_mask'].sum().item()
            labeled_mask = g.ndata['labeled_mask'].sum().item()

            print(f"è®­ç»ƒèŠ‚ç‚¹: {train_mask}")
            print(f"éªŒè¯èŠ‚ç‚¹: {valid_mask}")
            print(f"æµ‹è¯•èŠ‚ç‚¹: {test_mask}")
            print(f"æœ‰æ ‡ç­¾çš„è®­ç»ƒèŠ‚ç‚¹: {labeled_mask}")

            # å¦‚æœéªŒè¯é›†å’Œæµ‹è¯•é›†å¤ªå°ï¼Œé‡æ–°åˆ’åˆ†
            if valid_mask == 0 or test_mask == 0:
                print("è­¦å‘Š: éªŒè¯é›†æˆ–æµ‹è¯•é›†ä¸ºç©ºï¼Œé‡æ–°åˆ’åˆ†æ•°æ®é›†...")
                g = re_split_dataset(g)
        else:
            print("åŸå§‹å›¾æ•°æ®ä¸å­˜åœ¨ï¼Œé‡æ–°æ„å»º...")
            graph_builder = FB15KETGraphBuilder()
            g, entity_id_map, relation_id_map = graph_builder.build_heterogeneous_graph(
                use_relation_types=False
            )

            # ä¿å­˜æ˜ å°„
            mapping_data = {
                'entity_id_map': entity_id_map,
                'relation_id_map': relation_id_map,
                'category_names': data_loader.category_names
            }
            torch.save(mapping_data, 'processed_data/fb15ket_mappings.pt')

        # ============================================
        # 3. åˆ›å»ºé€‚åˆç»´åº¦çš„æ¨¡å‹
        # ============================================
        print("\n[3/7] åˆ›å»ºå¢å¼ºæ¨¡å‹...")
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"ä½¿ç”¨è®¾å¤‡: {device}")

        # è·å–å®é™…çš„è¾“å…¥ç»´åº¦
        feature_dim = g.ndata['feat'].shape[1]
        print(f"å®é™…ç‰¹å¾ç»´åº¦: {feature_dim}")

        model = SimpleImprovedModel(
            feature_dim=feature_dim,  # ä½¿ç”¨å®é™…çš„137ç»´
            hidden_dim=256,
            out_dim=128,
            num_classes=9
        )
        '''
        # åˆ›å»ºé€‚åˆç»´åº¦çš„æ¨¡å‹
        if feature_dim == 137:
            print("æ£€æµ‹åˆ°137ç»´ç‰¹å¾ï¼Œä½¿ç”¨é€‚é…æ¨¡å‹...")
            model = DimensionAdaptiveModel(feature_dim)
        else:
            print(f"ä½¿ç”¨æ ‡å‡†å¢å¼ºæ¨¡å‹ï¼Œç‰¹å¾ç»´åº¦: {feature_dim}")
            model = EnhancedFB15KETXGradNet(
                feature_dim=feature_dim,
                hidden_dim=256,
                out_dim=128,
                num_classes=9,
                num_prototypes_per_class=3,
                dropout_rate=0.5
            )
        '''
        # æ‰“å°æ¨¡å‹ä¿¡æ¯
        print("\næ¨¡å‹æ¶æ„:")
        print(model)

        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        print(f"\næ¨¡å‹å‚æ•°æ€»æ•°: {total_params:,}")
        print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

        # ============================================
        # 4. ä¿®å¤æ•°æ®é›†åˆ’åˆ†é—®é¢˜
        # ============================================
        print("\n[4/7] å‡†å¤‡è®­ç»ƒæ•°æ®...")

        # æ£€æŸ¥å½“å‰çš„æ•°æ®é›†åˆ’åˆ†
        train_mask = g.ndata['train_mask']
        valid_mask = g.ndata['valid_mask']
        test_mask = g.ndata['test_mask']
        labeled_mask = g.ndata['labeled_mask']

        print(f"æ•°æ®é›†åˆ’åˆ†ç»Ÿè®¡:")
        print(f"  è®­ç»ƒèŠ‚ç‚¹: {train_mask.sum().item()}")
        print(f"  éªŒè¯èŠ‚ç‚¹: {valid_mask.sum().item()}")
        print(f"  æµ‹è¯•èŠ‚ç‚¹: {test_mask.sum().item()}")
        print(f"  æœ‰æ ‡ç­¾çš„è®­ç»ƒèŠ‚ç‚¹: {labeled_mask.sum().item()}")

        # å¦‚æœéªŒè¯é›†å’Œæµ‹è¯•é›†å¤ªå°ï¼Œé‡æ–°åˆ’åˆ†
        if valid_mask.sum().item() < 100 or test_mask.sum().item() < 100:
            print("æ•°æ®é›†åˆ’åˆ†ä¸åˆç†ï¼Œé‡æ–°åˆ’åˆ†...")
            g = re_split_dataset(g)

        # æ£€æŸ¥æœ‰æ ‡ç­¾çš„èŠ‚ç‚¹
        labeled_indices = torch.where(labeled_mask)[0]
        print(f"æœ‰æ ‡ç­¾çš„èŠ‚ç‚¹ç´¢å¼•æ•°é‡: {len(labeled_indices)}")

        if len(labeled_indices) == 0:
            print("é”™è¯¯: æ²¡æœ‰æœ‰æ ‡ç­¾çš„è®­ç»ƒèŠ‚ç‚¹!")
            return None, None, None

        # ============================================
        # 5. è®­ç»ƒå¢å¼ºæ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼Œé¿å…å¤æ‚é”™è¯¯ï¼‰
        # ============================================
        print("\n[5/7] è®­ç»ƒå¢å¼ºæ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰...")

        # ä½¿ç”¨ç®€åŒ–è®­ç»ƒå™¨
        trainer = SimpleTrainer(model, g, device=device)

        # å…ˆæµ‹è¯•ä¸€æ¬¡å‰å‘ä¼ æ’­
        print("æµ‹è¯•å‰å‘ä¼ æ’­...")
        try:
            test_features = g.ndata['feat'].to(device)
            test_output = model(test_features)
            print(f"å‰å‘ä¼ æ’­æµ‹è¯•æˆåŠŸ!")
            print(f"è¾“å…¥ç»´åº¦: {test_features.shape}")
            print(f"è¾“å‡ºç»´åº¦: {test_output.shape}")
        except Exception as e:
            print(f"å‰å‘ä¼ æ’­æµ‹è¯•å¤±è´¥: {e}")
            print("è°ƒè¯•æ¨¡å‹æ¶æ„...")
            return None, None, None

        # è®­ç»ƒ
        train_losses, val_accuracies = trainer.train(
            epochs=100,  # å…ˆè®­ç»ƒ100ä¸ªepoch
            lr=0.001,
            weight_decay=1e-4
        )

        # ============================================
        # 6. è¯„ä¼°æ¨¡å‹
        # ============================================
        print("\n[6/7] è¯„ä¼°æ¨¡å‹...")
        results = trainer.test(save_results=True)

        # ============================================
        # 7. å°è¯•è¿›ä¸€æ­¥ä¼˜åŒ–ï¼ˆå¦‚æœç¬¬ä¸€æ¬¡è®­ç»ƒæˆåŠŸï¼‰
        # ============================================
        if results and results.get('train_acc', 0) > 0.5:
            print("\n[7/7] è¿›ä¸€æ­¥ä¼˜åŒ–...")

            # ä¿å­˜ç¬¬ä¸€æ¬¡è®­ç»ƒç»“æœ
            first_train_acc = results.get('train_acc', 0)
            first_test_acc = results.get('test_acc', 0)

            print(f"ç¬¬ä¸€æ¬¡è®­ç»ƒç»“æœ: è®­ç»ƒé›†={first_train_acc:.4f}, æµ‹è¯•é›†={first_test_acc:.4f}")

            # å°è¯•ç»§ç»­è®­ç»ƒæˆ–ä½¿ç”¨æ›´å¤æ‚çš„æ¨¡å‹
            if first_train_acc < 0.65:
                print("å°è¯•ç»§ç»­è®­ç»ƒ...")
                # å¯ä»¥ç»§ç»­è®­ç»ƒæˆ–è°ƒæ•´è¶…å‚æ•°

        print("\n" + "=" * 80)
        print("æ”¹è¿›è®­ç»ƒæµç¨‹å®Œæˆï¼")
        print("=" * 80)

        return model, g, results

    except Exception as e:
        print(f"ç¨‹åºè¿è¡Œå‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None


def re_split_dataset(g, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):
    """é‡æ–°åˆ’åˆ†æ•°æ®é›†"""
    print("é‡æ–°åˆ’åˆ†æ•°æ®é›†...")

    num_nodes = g.num_nodes()
    labels = g.ndata['label']

    # æ‰¾åˆ°æœ‰æ ‡ç­¾çš„èŠ‚ç‚¹
    labeled_indices = torch.where(labels != -1)[0]
    num_labeled = len(labeled_indices)

    print(f"æœ‰æ ‡ç­¾çš„èŠ‚ç‚¹æ•°: {num_labeled}")

    if num_labeled == 0:
        print("é”™è¯¯: æ²¡æœ‰æœ‰æ ‡ç­¾çš„èŠ‚ç‚¹!")
        return g

    # éšæœºæ‰“ä¹±
    shuffled_indices = labeled_indices[torch.randperm(num_labeled)]

    # è®¡ç®—åˆ’åˆ†ç‚¹
    train_end = int(num_labeled * train_ratio)
    val_end = train_end + int(num_labeled * val_ratio)

    # åˆ›å»ºæ©ç 
    train_mask = torch.zeros(num_nodes, dtype=torch.bool)
    valid_mask = torch.zeros(num_nodes, dtype=torch.bool)
    test_mask = torch.zeros(num_nodes, dtype=torch.bool)
    labeled_mask = torch.zeros(num_nodes, dtype=torch.bool)

    train_mask[shuffled_indices[:train_end]] = True
    valid_mask[shuffled_indices[train_end:val_end]] = True
    test_mask[shuffled_indices[val_end:]] = True
    labeled_mask[shuffled_indices[:train_end]] = True

    # æ›´æ–°å›¾çš„æ©ç 
    g.ndata['train_mask'] = train_mask
    g.ndata['valid_mask'] = valid_mask
    g.ndata['test_mask'] = test_mask
    g.ndata['labeled_mask'] = labeled_mask

    print(f"é‡æ–°åˆ’åˆ†ç»“æœ:")
    print(f"  è®­ç»ƒé›†: {train_mask.sum().item()} èŠ‚ç‚¹")
    print(f"  éªŒè¯é›†: {valid_mask.sum().item()} èŠ‚ç‚¹")
    print(f"  æµ‹è¯•é›†: {test_mask.sum().item()} èŠ‚ç‚¹")
    print(f"  æœ‰æ ‡ç­¾çš„è®­ç»ƒèŠ‚ç‚¹: {labeled_mask.sum().item()} èŠ‚ç‚¹")

    return g


class DimensionAdaptiveModel(nn.Module):
    """ç»´åº¦è‡ªé€‚åº”æ¨¡å‹ï¼ˆä¸“é—¨å¤„ç†137ç»´ç‰¹å¾ï¼‰"""

    def __init__(self, input_dim=137, hidden_dim=256, out_dim=128, num_classes=9):
        super().__init__()

        print(f"åˆ›å»ºç»´åº¦è‡ªé€‚åº”æ¨¡å‹: è¾“å…¥={input_dim}, éšè—={hidden_dim}, è¾“å‡º={out_dim}")

        # 1. è‡ªé€‚åº”è¾“å…¥å±‚
        self.input_layer = nn.Linear(input_dim, hidden_dim)

        # 2. ä¸­é—´å±‚
        self.hidden_layers = nn.Sequential(
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),

            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.3),

            nn.Linear(hidden_dim // 2, out_dim),
            nn.BatchNorm1d(out_dim),
            nn.ReLU(),
            nn.Dropout(0.2)
        )

        # 3. åˆ†ç±»å¤´
        self.classifier = nn.Sequential(
            nn.Linear(out_dim, out_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(out_dim // 2, num_classes)
        )

        # 4. åŸå‹ç½‘ç»œï¼ˆç®€åŒ–ï¼‰
        self.prototype_net = SimplePrototypeNetwork(out_dim, num_classes)

        # åˆå§‹åŒ–æƒé‡
        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        # è°ƒè¯•ä¿¡æ¯
        if hasattr(self, 'debug') and self.debug:
            print(f"è¾“å…¥ç»´åº¦: {x.shape}")
            print(f"è¾“å…¥å±‚æƒé‡ç»´åº¦: {self.input_layer.weight.shape}")

        x = self.input_layer(x)
        x = self.hidden_layers(x)
        return x

    def classify(self, embeddings, indices, update_prototypes=False, labels=None):
        """åˆ†ç±»"""
        if embeddings is None or len(indices) == 0:
            return None, None, None

        # è·å–æŒ‡å®šèŠ‚ç‚¹çš„åµŒå…¥
        node_embeddings = embeddings[indices]

        # åŸå‹åˆ†ç±»
        proto_probs, similarities, class_similarities = self.prototype_net(node_embeddings)

        # åˆ†ç±»å™¨åˆ†ç±»
        logits = self.classifier(node_embeddings)
        classifier_probs = F.softmax(logits, dim=1)

        # èåˆï¼ˆåŸå‹ç½‘ç»œæƒé‡0.6ï¼Œåˆ†ç±»å™¨æƒé‡0.4ï¼‰
        alpha = 0.6
        combined_probs = alpha * proto_probs + (1 - alpha) * classifier_probs

        # æ›´æ–°åŸå‹
        if update_prototypes and labels is not None:
            self.prototype_net.update_prototypes(node_embeddings, labels[indices])

        # é¢„æµ‹ç±»åˆ«
        _, predicted_classes = torch.max(combined_probs, dim=1)

        return combined_probs, predicted_classes, {
            'proto_probs': proto_probs,
            'classifier_probs': classifier_probs,
            'similarities': similarities
        }

    def simple_classify(self, embeddings, indices):
        """ç®€åŒ–åˆ†ç±»"""
        if embeddings is None or len(indices) == 0:
            return None, None

        node_embeddings = embeddings[indices]
        logits = self.classifier(node_embeddings)
        probs = F.softmax(logits, dim=1)
        _, predicted_classes = torch.max(probs, dim=1)

        return probs, predicted_classes


class SimplePrototypeNetwork(nn.Module):
    """ç®€åŒ–åŸå‹ç½‘ç»œ"""

    def __init__(self, feature_dim, num_classes, num_prototypes_per_class=2):
        super().__init__()
        self.feature_dim = feature_dim
        self.num_classes = num_classes
        self.num_prototypes = num_classes * num_prototypes_per_class

        # å¯å­¦ä¹ çš„åŸå‹
        self.prototypes = nn.Parameter(torch.randn(self.num_prototypes, feature_dim))

        # åŸå‹åˆ°ç±»åˆ«çš„æ˜ å°„
        self.prototype_to_class = torch.repeat_interleave(
            torch.arange(num_classes), num_prototypes_per_class
        )

        # åˆå§‹åŒ–
        nn.init.xavier_uniform_(self.prototypes)

    def forward(self, features):
        # è®¡ç®—è·ç¦»
        distances = torch.cdist(features, self.prototypes, p=2)

        # è½¬æ¢ä¸ºç›¸ä¼¼åº¦
        similarities = torch.exp(-distances)

        # æŒ‰ç±»åˆ«èšåˆ
        class_similarities = torch.zeros(features.size(0), self.num_classes, device=features.device)

        for c in range(self.num_classes):
            proto_indices = (self.prototype_to_class == c).nonzero(as_tuple=True)[0]
            if len(proto_indices) > 0:
                if len(proto_indices) == 1:
                    class_similarities[:, c] = similarities[:, proto_indices[0]]
                else:
                    class_similarities[:, c] = similarities[:, proto_indices].max(dim=1).values

        # è®¡ç®—æ¦‚ç‡
        probs = F.softmax(class_similarities, dim=1)

        return probs, similarities, class_similarities

    def update_prototypes(self, features, labels):
        with torch.no_grad():
            for c in range(self.num_classes):
                mask = (labels == c)
                if mask.sum() > 0:
                    class_features = features[mask]
                    proto_indices = (self.prototype_to_class == c).nonzero(as_tuple=True)[0]

                    if len(class_features) >= len(proto_indices):
                        # å‡åŒ€é€‰æ‹©æ ·æœ¬ä½œä¸ºåŸå‹
                        step = len(class_features) // len(proto_indices)
                        for i, idx in enumerate(proto_indices):
                            sample_idx = min(i * step, len(class_features) - 1)
                            self.prototypes.data[idx] = 0.9 * self.prototypes.data[idx] + 0.1 * class_features[
                                sample_idx]


# ç®€åŒ–ç‰ˆæ”¹è¿›å‡½æ•°
def simple_improvement():
    """ç®€åŒ–ç‰ˆæ”¹è¿›ï¼ˆä¿®å¤ç‰ˆï¼‰"""
    print("=" * 80)
    print("FB15KETå®ä½“åˆ†ç±»ç³»ç»Ÿ - ç®€åŒ–æ”¹è¿›ç‰ˆ")
    print("=" * 80)

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)

    try:
        # 1. åŠ è½½å›¾æ•°æ®
        print("\n[1] åŠ è½½å›¾æ•°æ®...")
        if not os.path.exists('processed_data/fb15ket_graph.bin'):
            print("å›¾æ•°æ®ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡ŒåŸå§‹ç‰ˆæœ¬æ„å»ºå›¾")
            return None, None, None

        g_list, _ = dgl.load_graphs('processed_data/fb15ket_graph.bin')
        g = g_list[0]

        # æ£€æŸ¥æ•°æ®é›†åˆ’åˆ†
        train_mask = g.ndata['train_mask'].sum().item()
        valid_mask = g.ndata['valid_mask'].sum().item()
        test_mask = g.ndata['test_mask'].sum().item()

        print(f"æ•°æ®é›†åˆ’åˆ†:")
        print(f"  è®­ç»ƒèŠ‚ç‚¹: {train_mask}")
        print(f"  éªŒè¯èŠ‚ç‚¹: {valid_mask}")
        print(f"  æµ‹è¯•èŠ‚ç‚¹: {test_mask}")

        # 2. åˆ›å»ºåˆé€‚çš„æ¨¡å‹
        print("\n[2] åˆ›å»ºæ”¹è¿›æ¨¡å‹...")
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        feature_dim = g.ndata['feat'].shape[1]

        print(f"ç‰¹å¾ç»´åº¦: {feature_dim}")

        # æ ¹æ®ç‰¹å¾ç»´åº¦é€‰æ‹©åˆé€‚çš„æ¨¡å‹
        if feature_dim == 10:
            # åŸå§‹ç‰¹å¾ç»´åº¦
            hidden_dim = 128
            out_dim = 64
        elif feature_dim == 137:
            # å¢å¼ºç‰¹å¾ç»´åº¦
            hidden_dim = 256
            out_dim = 128
        else:
            # è‡ªé€‚åº”
            hidden_dim = min(256, feature_dim * 2)
            out_dim = min(128, feature_dim)

        print(f"ä½¿ç”¨æ¨¡å‹é…ç½®: hidden_dim={hidden_dim}, out_dim={out_dim}")

        model = SimpleImprovedModel(feature_dim, hidden_dim, out_dim, 9)
        model = model.to(device)

        # 3. è®­ç»ƒ
        print("\n[3] è®­ç»ƒæ”¹è¿›æ¨¡å‹...")
        trainer = SimpleTrainer(model, g, device=device)

        # å…ˆæµ‹è¯•å‰å‘ä¼ æ’­
        print("æµ‹è¯•å‰å‘ä¼ æ’­...")
        try:
            test_features = g.ndata['feat'].to(device)
            test_output = model(test_features)
            print(f"âœ“ å‰å‘ä¼ æ’­æµ‹è¯•æˆåŠŸ")
            print(f"  è¾“å…¥: {test_features.shape}")
            print(f"  è¾“å‡º: {test_output.shape}")
        except Exception as e:
            print(f"âœ— å‰å‘ä¼ æ’­å¤±è´¥: {e}")
            return None, None, None

        # å¼€å§‹è®­ç»ƒ
        print("\nå¼€å§‹è®­ç»ƒ...")
        train_losses, val_accuracies = trainer.train(
            epochs=100,
            lr=0.001,
            weight_decay=1e-4
        )

        # 4. è¯„ä¼°
        print("\n[4] è¯„ä¼°æ¨¡å‹...")
        results = trainer.test(save_results=True)

        return model, g, results

    except Exception as e:
        print(f"ç®€åŒ–æ”¹è¿›å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None


class SimpleImprovedModel(nn.Module):
    """ç®€å•æ”¹è¿›æ¨¡å‹ï¼ˆä¿®å¤137ç»´é—®é¢˜ï¼‰"""

    def __init__(self, feature_dim, hidden_dim=128, out_dim=64, num_classes=9):
        super().__init__()

        print(f"åˆ›å»ºç®€å•æ”¹è¿›æ¨¡å‹: input={feature_dim}, hidden={hidden_dim}, output={out_dim}")

        # è‡ªé€‚åº”ç‰¹å¾è½¬æ¢
        self.feature_transform = nn.Sequential(
            nn.Linear(feature_dim, hidden_dim),
            nn.BatchNorm1d(hidden_dim),
            nn.ReLU(),
            nn.Dropout(0.3),

            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.BatchNorm1d(hidden_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.2),

            nn.Linear(hidden_dim // 2, out_dim),
            nn.BatchNorm1d(out_dim),
            nn.ReLU(),
            nn.Dropout(0.1)
        )

        # åˆ†ç±»å™¨
        self.classifier = nn.Sequential(
            nn.Linear(out_dim, out_dim // 2),
            nn.ReLU(),
            nn.Dropout(0.1),
            nn.Linear(out_dim // 2, num_classes)
        )

        # åˆå§‹åŒ–
        self._init_weights()

    def _init_weights(self):
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                if m.bias is not None:
                    nn.init.zeros_(m.bias)
            elif isinstance(m, nn.BatchNorm1d):
                nn.init.ones_(m.weight)
                nn.init.zeros_(m.bias)

    def forward(self, x):
        return self.feature_transform(x)

    def classify(self, embeddings, indices, update_prototypes=False, labels=None):
        if embeddings is None or len(indices) == 0:
            return None, None, None

        node_embeddings = embeddings[indices]
        logits = self.classifier(node_embeddings)
        probs = F.softmax(logits, dim=1)
        _, predicted_classes = torch.max(probs, dim=1)

        return probs, predicted_classes, {}

    def simple_classify(self, embeddings, indices):
        return self.classify(embeddings, indices)





# ============================================
# è¾…åŠ©å‡½æ•°
# ============================================

def build_enhanced_features(triplets_by_entity):
    """æ„å»ºå¢å¼ºç‰¹å¾"""
    print("æ„å»ºå¢å¼ºç‰¹å¾...")

    # åŠ è½½å®ä½“æ•°æ®
    entity_df = pd.read_csv('data/FB15KET/Entity_All_typed.csv')

    enhanced_features = {}

    for _, row in entity_df.iterrows():
        eid = row['entity_id']

        # 1. åŸºæœ¬ç‰¹å¾ï¼š9ä¸ªç±»åˆ«å¾—åˆ†
        base_features = []
        for i in range(1, 10):
            col_name = f'category_{i}_score'
            if col_name in row and not pd.isna(row[col_name]):
                base_features.append(float(row[col_name]))
            else:
                base_features.append(0.0)

        # 2. ç»“æ„ç‰¹å¾
        structural_features = extract_structural_features(eid, triplets_by_entity)

        # 3. ç»„åˆæ‰€æœ‰ç‰¹å¾
        all_features = base_features + structural_features

        enhanced_features[eid] = all_features

    print(f"æ„å»ºäº† {len(enhanced_features)} ä¸ªå®ä½“çš„å¢å¼ºç‰¹å¾")
    print(f"ç‰¹å¾ç»´åº¦: {len(next(iter(enhanced_features.values())))}")

    return enhanced_features


def extract_structural_features(entity_id, triplets_by_entity):
    """æå–ç»“æ„ç‰¹å¾"""
    if entity_id not in triplets_by_entity:
        return [0.0] * 5  # è¿”å›é»˜è®¤ç‰¹å¾

    relations = triplets_by_entity[entity_id]

    # è®¡ç®—å„ç§ç»“æ„ç‰¹å¾
    out_degree = sum(1 for d, _, _ in relations if d == 'out')
    in_degree = sum(1 for d, _, _ in relations if d == 'in')
    total_degree = len(relations)

    # å…³ç³»ç±»å‹å¤šæ ·æ€§
    rel_types = set(rel for _, rel, _ in relations)
    rel_diversity = len(rel_types) / (total_degree + 1e-8)

    # å½’ä¸€åŒ–ç‰¹å¾
    features = [
        min(out_degree / 100.0, 1.0),  # å½’ä¸€åŒ–åˆ°[0,1]
        min(in_degree / 100.0, 1.0),
        min(total_degree / 200.0, 1.0),
        rel_diversity,
        min(len(rel_types) / 50.0, 1.0)
    ]

    return features


def analyze_results(results, train_losses, val_accuracies):
    """åˆ†æè®­ç»ƒç»“æœ"""
    print("\næ€§èƒ½åˆ†ææŠ¥å‘Š:")
    print("-" * 60)

    train_acc = results.get('train_acc', 0)
    val_acc = results.get('valid_acc', 0)
    test_acc = results.get('test_acc', 0)

    print(f"è®­ç»ƒé›†å‡†ç¡®ç‡: {train_acc:.4f}")
    print(f"éªŒè¯é›†å‡†ç¡®ç‡: {val_acc:.4f}")
    print(f"æµ‹è¯•é›†å‡†ç¡®ç‡: {test_acc:.4f}")

    # åˆ†æè¿‡æ‹Ÿåˆ/æ¬ æ‹Ÿåˆ
    if train_acc > val_acc + 0.05:
        print("âš ï¸  å¯èƒ½å­˜åœ¨è¿‡æ‹Ÿåˆ (è®­ç»ƒé›† >> éªŒè¯é›†)")
        print("   å»ºè®®: å¢åŠ dropout, æ•°æ®å¢å¼º, æ—©åœ")
    elif train_acc < val_acc:
        print("âš ï¸  å¯èƒ½å­˜åœ¨æ¬ æ‹Ÿåˆ (è®­ç»ƒé›† < éªŒè¯é›†)")
        print("   å»ºè®®: å¢åŠ æ¨¡å‹å¤æ‚åº¦, è®­ç»ƒæ›´å¤šepoch, æ•°æ®å¢å¼º")
    else:
        print("âœ“ è®­ç»ƒé›†å’ŒéªŒè¯é›†æ€§èƒ½å¹³è¡¡")

    # æ³›åŒ–èƒ½åŠ›
    if abs(val_acc - test_acc) < 0.02:
        print("âœ“ æ³›åŒ–èƒ½åŠ›è‰¯å¥½ (éªŒè¯é›† â‰ˆ æµ‹è¯•é›†)")
    else:
        print("âš ï¸  æ³›åŒ–èƒ½åŠ›æœ‰å¾…æå‡")

    # ç»å¯¹æ€§èƒ½
    if test_acc > 0.7:
        print("ğŸ‰ æ€§èƒ½ä¼˜ç§€ (>70%)")
    elif test_acc > 0.6:
        print("ğŸ‘ æ€§èƒ½è‰¯å¥½ (60-70%)")
    elif test_acc > 0.5:
        print("ğŸ‘Œ æ€§èƒ½ä¸€èˆ¬ (50-60%)")
    else:
        print("ğŸ”§ éœ€è¦å¤§å¹…æ”¹è¿› (<50%)")

    # è®­ç»ƒè¿‡ç¨‹åˆ†æ
    if train_losses:
        final_loss = train_losses[-1]
        print(f"\nè®­ç»ƒæŸå¤±åˆ†æ:")
        print(f"  æœ€ç»ˆè®­ç»ƒæŸå¤±: {final_loss:.4f}")
        if len(train_losses) > 1:
            loss_decrease = train_losses[0] - final_loss
            print(f"  æ€»æŸå¤±ä¸‹é™: {loss_decrease:.4f}")
            if loss_decrease < 0.1:
                print("  è­¦å‘Š: æŸå¤±ä¸‹é™ä¸è¶³ï¼Œå¯èƒ½å­¦ä¹ ç‡å¤ªå°æˆ–æ¨¡å‹å¤ªç®€å•")


# ============================================
# ç®€åŒ–ç‰ˆæ”¹è¿›ï¼ˆé€æ­¥å®æ–½ï¼‰
# ============================================

def simple_improvement():
    """ç®€åŒ–ç‰ˆæ”¹è¿›ï¼šåªä¿®æ”¹æœ€å®¹æ˜“å®ç°çš„"""
    print("=" * 80)
    print("FB15KETå®ä½“åˆ†ç±»ç³»ç»Ÿ - ç®€åŒ–æ”¹è¿›ç‰ˆ")
    print("=" * 80)

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)

    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    os.makedirs('predictions', exist_ok=True)

    try:
        # 1. åŠ è½½ç°æœ‰å›¾æ•°æ®
        print("\n[1] åŠ è½½å›¾æ•°æ®...")
        g_list, _ = dgl.load_graphs('processed_data/fb15ket_graph.bin')
        g = g_list[0]

        # 2. åˆ›å»ºå¢å¼ºæ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰
        print("\n[2] åˆ›å»ºå¢å¼ºæ¨¡å‹ï¼ˆç®€åŒ–ç‰ˆï¼‰...")
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        feature_dim = g.ndata['feat'].shape[1]

        # ç®€åŒ–ç‰ˆå¢å¼ºæ¨¡å‹
        class SimplifiedEnhancedModel(SimpleFB15KETXGradNet):
            def __init__(self, feature_dim, hidden_dim=256, out_dim=128, num_classes=9):
                super().__init__(feature_dim, hidden_dim, out_dim, num_classes)

                # å¢å¼ºç‰¹å¾è½¬æ¢å±‚
                self.enhanced_transform = nn.Sequential(
                    nn.Linear(feature_dim, hidden_dim),
                    nn.BatchNorm1d(hidden_dim),
                    nn.ReLU(),
                    nn.Dropout(0.3),
                    nn.Linear(hidden_dim, out_dim),
                    nn.BatchNorm1d(out_dim),
                    nn.ReLU(),
                    nn.Dropout(0.2)
                )

                # å¢å¼ºåˆ†ç±»å¤´
                self.enhanced_classifier = nn.Sequential(
                    nn.Linear(out_dim, hidden_dim // 2),
                    nn.ReLU(),
                    nn.Dropout(0.2),
                    nn.Linear(hidden_dim // 2, num_classes)
                )

                # æ›´å¥½çš„åˆå§‹åŒ–
                self._init_enhanced_weights()

            def _init_enhanced_weights(self):
                for m in self.modules():
                    if isinstance(m, nn.Linear):
                        nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
                        if m.bias is not None:
                            nn.init.zeros_(m.bias)
                    elif isinstance(m, nn.BatchNorm1d):
                        nn.init.ones_(m.weight)
                        nn.init.zeros_(m.bias)

            def forward(self, node_features):
                # ä½¿ç”¨å¢å¼ºçš„ç‰¹å¾è½¬æ¢
                return self.enhanced_transform(node_features)

            def classify(self, node_embeddings, node_ids, update_prototypes=False, labels=None):
                # ä½¿ç”¨å¢å¼ºçš„åˆ†ç±»å™¨
                embeddings = node_embeddings[node_ids]
                logits = self.enhanced_classifier(embeddings)
                probs = F.softmax(logits, dim=1)
                _, predicted_classes = torch.max(probs, dim=1)

                return probs, predicted_classes, {}

        model = SimplifiedEnhancedModel(feature_dim)

        # 3. æ”¹è¿›è®­ç»ƒå™¨
        print("\n[3] æ”¹è¿›è®­ç»ƒ...")

        class ImprovedSimpleTrainer(SimpleTrainer):
            def train(self, epochs=100, lr=0.001):
                optimizer = torch.optim.AdamW(
                    self.model.parameters(),
                    lr=lr,
                    weight_decay=1e-4,
                    betas=(0.9, 0.999)
                )

                scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)

                best_val_acc = 0
                train_losses = []
                val_accuracies = []

                print("\nå¼€å§‹æ”¹è¿›è®­ç»ƒ...")
                print("-" * 80)

                for epoch in range(epochs):
                    self.model.train()

                    try:
                        node_embeddings = self.model(self.features)
                        probs, preds, _ = self.model.classify(
                            node_embeddings, self.labeled_indices,
                            update_prototypes=False,
                            labels=self.labels[self.labeled_indices]
                        )

                        if probs is None:
                            continue

                        # è®¡ç®—æŸå¤±
                        cls_loss = F.cross_entropy(probs, self.labels[self.labeled_indices])

                        # æ·»åŠ æ ‡ç­¾å¹³æ»‘
                        smooth_labels = torch.full_like(probs, 0.1 / 8.0)
                        smooth_labels.scatter_(1, self.labels[self.labeled_indices].unsqueeze(1), 0.9)
                        smooth_loss = F.kl_div(F.log_softmax(probs, dim=1), smooth_labels, reduction='batchmean')

                        total_loss = 0.8 * cls_loss + 0.2 * smooth_loss

                        # åå‘ä¼ æ’­
                        optimizer.zero_grad()
                        total_loss.backward()

                        # æ¢¯åº¦è£å‰ª
                        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

                        optimizer.step()
                        scheduler.step()

                        train_losses.append(total_loss.item())

                    except Exception as e:
                        print(f"Epoch {epoch + 1} è®­ç»ƒå‡ºé”™: {e}")
                        continue

                    # éªŒè¯
                    if (epoch + 1) % 5 == 0:
                        val_acc = self.evaluate(mode='valid')
                        val_accuracies.append(val_acc)

                        print(f"Epoch {epoch + 1:3d}/{epochs} | "
                              f"Loss: {total_loss.item():.4f} | "
                              f"Val Acc: {val_acc:.4f}")

                        if val_acc > best_val_acc:
                            best_val_acc = val_acc
                            torch.save({
                                'epoch': epoch,
                                'model_state_dict': self.model.state_dict(),
                                'val_acc': val_acc
                            }, 'improved_model.pth')

                print(f"\næœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc:.4f}")
                return train_losses, val_accuracies

        trainer = ImprovedSimpleTrainer(model, g, device=device)

        # è®­ç»ƒæ›´å¤šepochs
        train_losses, val_accuracies = trainer.train(epochs=100, lr=0.001)

        # 4. è¯„ä¼°
        print("\n[4] è¯„ä¼°æ”¹è¿›æ¨¡å‹...")
        results = trainer.test(save_results=True)

        # 5. æ¯”è¾ƒæ”¹è¿›æ•ˆæœ
        print("\n[5] æ”¹è¿›æ•ˆæœå¯¹æ¯”:")
        print("-" * 60)
        print("åŸå§‹æ¨¡å‹: è®­ç»ƒé›†å‡†ç¡®ç‡ ~0.5731")
        print(f"æ”¹è¿›æ¨¡å‹: è®­ç»ƒé›†å‡†ç¡®ç‡ {results.get('train_acc', 0):.4f}")
        print(f"æå‡å¹…åº¦: {results.get('train_acc', 0) - 0.5731:.4f}")

        return model, g, results

    except Exception as e:
        print(f"ç®€åŒ–æ”¹è¿›å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None


# ============================================
# æ‰§è¡Œå‡½æ•°
# ============================================



def generate_interpretations(model, graph, category_names, device):
    """ç”Ÿæˆå¯è§£é‡Šæ€§åˆ†æ"""
    print("ç”Ÿæˆå¯è§£é‡Šæ€§åˆ†æ...")

    # é€‰æ‹©ä¸€äº›ç¤ºä¾‹èŠ‚ç‚¹è¿›è¡Œåˆ†æ
    test_indices = torch.where(graph.ndata['test_mask'] & (graph.ndata['label'] != -1))[0]

    if len(test_indices) == 0:
        print("æ²¡æœ‰æ‰¾åˆ°æµ‹è¯•èŠ‚ç‚¹è¿›è¡Œåˆ†æ")
        return

    # éšæœºé€‰æ‹©10ä¸ªèŠ‚ç‚¹
    if len(test_indices) > 10:
        sample_indices = test_indices[torch.randperm(len(test_indices))[:10]]
    else:
        sample_indices = test_indices

    # è·å–ç‰¹å¾
    features = graph.ndata['feat'].to(device)

    interpretations = []

    for idx in sample_indices:
        interpretation = model.get_interpretation(
            idx.item(), model(features), category_names
        )

        if interpretation is not None:
            interpretations.append({
                'node_id': idx.item(),
                'interpretation': interpretation
            })

    # ä¿å­˜è§£é‡Šç»“æœ
    if interpretations:
        import json

        # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„æ ¼å¼
        serializable_interpretations = []
        for item in interpretations:
            node_id = item['node_id']
            interp = item['interpretation']

            serializable = {
                'node_id': node_id,
                'predicted_class': interp['predicted_class'],
                'class_similarities': interp['class_similarities'],
                'structure_contributions': {
                    k: (float(v) if isinstance(v, torch.Tensor) else v)
                    for k, v in interp['structure_contributions'].items()
                }
            }
            serializable_interpretations.append(serializable)

        # ä¿å­˜ä¸ºJSON
        with open('predictions/interpretations.json', 'w', encoding='utf-8') as f:
            json.dump(serializable_interpretations, f, ensure_ascii=False, indent=2)

        print(f"å¯è§£é‡Šæ€§åˆ†æå·²ä¿å­˜åˆ°: predictions/interpretations.json")

        # æ‰“å°ä¸€ä¸ªç¤ºä¾‹
        print("\nç¤ºä¾‹è§£é‡Šåˆ†æ:")
        print("-" * 60)
        example = serializable_interpretations[0]
        print(f"èŠ‚ç‚¹ID: {example['node_id']}")
        print(f"é¢„æµ‹ç±»åˆ«: {example['predicted_class']['name']} "
              f"(æ¦‚ç‡: {example['predicted_class']['probability']:.3f})")

        print("\nä¸å„ç±»åˆ«çš„ç›¸ä¼¼åº¦:")
        for class_name, similarity in example['class_similarities'].items():
            print(f"  {class_name}: {similarity:.4f}")

        print("\nç»“æ„è´¡çŒ®:")
        for component, value in example['structure_contributions'].items():
            print(f"  {component}: {value:.4f}")


class VisualizationTool:
    """å¯è§†åŒ–å·¥å…·ç±»"""

    def __init__(self, model, graph, category_names):
        self.model = model
        self.graph = graph
        self.category_names = category_names

    def plot_training_history(self, train_losses, val_accuracies):
        """ç»˜åˆ¶è®­ç»ƒå†å²"""
        plt.figure(figsize=(12, 5))

        # è®­ç»ƒæŸå¤±
        plt.subplot(1, 2, 1)
        plt.plot(train_losses)
        plt.title('Training Loss')
        plt.xlabel('Epoch')
        plt.ylabel('Loss')
        plt.grid(True, alpha=0.3)

        # éªŒè¯å‡†ç¡®ç‡
        plt.subplot(1, 2, 2)
        plt.plot(val_accuracies)
        plt.title('Validation Accuracy')
        plt.xlabel('Epoch (æ¯5è½®)')
        plt.ylabel('Accuracy')
        plt.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig('visualizations/training_history.png', dpi=150, bbox_inches='tight')
        plt.show()

    def plot_prototype_similarity(self):
        """å¯è§†åŒ–åŸå‹ç›¸ä¼¼åº¦"""
        self.model.eval()

        # è·å–åŸå‹
        prototypes = self.model.prototype_net.prototypes.detach().cpu().numpy()

        # è®¡ç®—åŸå‹é—´çš„ç›¸ä¼¼åº¦
        from sklearn.metrics.pairwise import cosine_similarity
        similarity_matrix = cosine_similarity(prototypes)

        plt.figure(figsize=(10, 8))
        plt.imshow(similarity_matrix, cmap='viridis', interpolation='nearest')
        plt.colorbar(label='ç›¸ä¼¼åº¦')
        plt.title('åŸå‹é—´ç›¸ä¼¼åº¦çŸ©é˜µ')
        plt.xlabel('åŸå‹ç´¢å¼•')
        plt.ylabel('åŸå‹ç´¢å¼•')

        # æ·»åŠ åŸå‹ç±»åˆ«æ ‡ç­¾
        num_prototypes_per_class = self.model.prototype_net.num_prototypes // 9
        for i in range(9):
            start_idx = i * num_prototypes_per_class
            plt.axhline(y=start_idx - 0.5, color='white', linestyle='--', alpha=0.5)
            plt.axvline(x=start_idx - 0.5, color='white', linestyle='--', alpha=0.5)

        plt.tight_layout()
        plt.savefig('visualizations/prototype_similarity.png', dpi=150, bbox_inches='tight')
        plt.show()

    def plot_class_distribution(self, predictions_df):
        """ç»˜åˆ¶ç±»åˆ«åˆ†å¸ƒ"""
        fig, axes = plt.subplots(1, 2, figsize=(14, 6))

        # çœŸå®ç±»åˆ«åˆ†å¸ƒ
        true_classes = predictions_df[predictions_df['true_class'] != -1]['true_class']
        true_counts = true_classes.value_counts().sort_index()

        axes[0].bar(range(len(true_counts)), true_counts.values)
        axes[0].set_title('çœŸå®ç±»åˆ«åˆ†å¸ƒ')
        axes[0].set_xlabel('ç±»åˆ«')
        axes[0].set_ylabel('å®ä½“æ•°é‡')
        axes[0].set_xticks(range(len(true_counts)))
        axes[0].set_xticklabels([self.category_names.get(i, i) for i in true_counts.index], rotation=45, ha='right')

        # é¢„æµ‹ç±»åˆ«åˆ†å¸ƒ
        pred_counts = predictions_df['predicted_class'].value_counts().sort_index()

        axes[1].bar(range(len(pred_counts)), pred_counts.values)
        axes[1].set_title('é¢„æµ‹ç±»åˆ«åˆ†å¸ƒ')
        axes[1].set_xlabel('ç±»åˆ«')
        axes[1].set_ylabel('å®ä½“æ•°é‡')
        axes[1].set_xticks(range(len(pred_counts)))
        axes[1].set_xticklabels([self.category_names.get(i, i) for i in pred_counts.index], rotation=45, ha='right')

        plt.tight_layout()
        plt.savefig('visualizations/class_distribution.png', dpi=150, bbox_inches='tight')
        plt.show()

    def plot_confusion_matrix(self, cm_path='predictions/confusion_matrix.csv'):
        """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
        if not os.path.exists(cm_path):
            return

        cm_df = pd.read_csv(cm_path, index_col=0)

        plt.figure(figsize=(12, 10))
        sns.heatmap(cm_df, annot=True, fmt='d', cmap='Blues')
        plt.title('æ··æ·†çŸ©é˜µ')
        plt.xlabel('é¢„æµ‹ç±»åˆ«')
        plt.ylabel('çœŸå®ç±»åˆ«')
        plt.tight_layout()
        plt.savefig('visualizations/confusion_matrix.png', dpi=150, bbox_inches='tight')
        plt.show()

    def generate_report(self, results, predictions_df):
        """ç”Ÿæˆå®Œæ•´æŠ¥å‘Š"""
        report = f"""
# FB15KETå®ä½“åˆ†ç±»ç³»ç»ŸæŠ¥å‘Š

## 1. æ¨¡å‹æ€§èƒ½
- è®­ç»ƒé›†å‡†ç¡®ç‡: {results.get('train_acc', 0):.4f}
- éªŒè¯é›†å‡†ç¡®ç‡: {results.get('valid_acc', 0):.4f}
- æµ‹è¯•é›†å‡†ç¡®ç‡: {results.get('test_acc', 0):.4f}

## 2. æ•°æ®ç»Ÿè®¡
- æ€»å®ä½“æ•°: {len(predictions_df)}
- æœ‰æ ‡ç­¾çš„å®ä½“: {len(predictions_df[predictions_df['has_label']])}
- æ— æ ‡ç­¾çš„å®ä½“: {len(predictions_df[~predictions_df['has_label']])}

## 3. ç±»åˆ«åˆ†å¸ƒ
"""

        # æ·»åŠ ç±»åˆ«ç»Ÿè®¡
        for class_id in range(1, 10):
            class_name = self.category_names.get(class_id, f"ç±»åˆ«{class_id}")
            true_count = len(predictions_df[(predictions_df['true_class'] == class_id)])
            pred_count = len(predictions_df[(predictions_df['predicted_class'] == class_id)])

            report += f"- {class_name}:\n"
            report += f"  çœŸå®æ•°é‡: {true_count}\n"
            report += f"  é¢„æµ‹æ•°é‡: {pred_count}\n"

        # ä¿å­˜æŠ¥å‘Š
        with open('predictions/system_report.md', 'w', encoding='utf-8') as f:
            f.write(report)

        print(f"ç³»ç»ŸæŠ¥å‘Šå·²ä¿å­˜åˆ°: predictions/system_report.md")

        return report


# ä½¿ç”¨ç¤ºä¾‹
def run_visualization():
    """è¿è¡Œå¯è§†åŒ–åˆ†æ"""
    # åŠ è½½æ•°æ®
    g, _ = dgl.load_graphs('processed_data/fb15ket_graph.bin')
    graph = g[0]

    mapping_data = torch.load('processed_data/fb15ket_mappings.pt')
    category_names = mapping_data['category_names']

    # åŠ è½½æ¨¡å‹
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    feature_dim = graph.ndata['feat'].shape[1]

    model = FB15KETXGradNet(graph, feature_dim)
    checkpoint = torch.load('models/best_model.pth', map_location=device)
    model.load_state_dict(checkpoint['model_state_dict'])
    model = model.to(device)

    # åŠ è½½é¢„æµ‹ç»“æœ
    predictions_df = pd.read_csv('predictions/fb15ket_predictions.csv')

    # åˆ›å»ºå¯è§†åŒ–å·¥å…·
    viz_tool = VisualizationTool(model, graph, category_names)

    # ç”Ÿæˆå¯è§†åŒ–
    print("ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    viz_tool.plot_class_distribution(predictions_df)
    viz_tool.plot_confusion_matrix()
    viz_tool.plot_prototype_similarity()

    # ç”ŸæˆæŠ¥å‘Š
    results = {
        'train_acc': predictions_df[predictions_df['in_train'] & predictions_df['has_label']]
        .apply(lambda row: row['predicted_class'] == row['true_class'], axis=1).mean(),
        'valid_acc': predictions_df[predictions_df['in_valid'] & predictions_df['has_label']]
        .apply(lambda row: row['predicted_class'] == row['true_class'], axis=1).mean(),
        'test_acc': predictions_df[predictions_df['in_test'] & predictions_df['has_label']]
        .apply(lambda row: row['predicted_class'] == row['true_class'], axis=1).mean()
    }

    viz_tool.generate_report(results, predictions_df)

    print("å¯è§†åŒ–åˆ†æå®Œæˆï¼")


if __name__ == "__main__":
    print("FB15KETå®ä½“åˆ†ç±»æ”¹è¿›ç³»ç»Ÿ")
    print("=" * 80)
    print("æ‚¨çš„ç‰¹å¾ç»´åº¦ä¸º137ï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†")
    print("=" * 80)

    print("\nä¿®å¤æ–¹æ¡ˆé€‰æ‹©:")
    print("1. æ‰§è¡Œ137ç»´ä¸“ç”¨ä¿®å¤ (æ¨è)")
    print("2. è¿è¡Œç®€åŒ–æ”¹è¿›ç‰ˆ")
    print("3. è¿è¡Œå®Œæ•´æ”¹è¿›ç‰ˆ")
    print("4. é€€å‡º")

    choice = input("\nè¯·è¾“å…¥é€‰æ‹© (1-4): ").strip()

    if choice == '1':
        print("\næ‰§è¡Œ137ç»´ä¸“ç”¨ä¿®å¤...")
        model, graph, results = fix_137_dimension_issue()
    elif choice == '2':
        print("\næ‰§è¡Œç®€åŒ–æ”¹è¿›ç‰ˆ...")
        model, graph, results = simple_improvement()
    elif choice == '3':
        print("\næ‰§è¡Œå®Œæ•´æ”¹è¿›ç‰ˆ...")
        model, graph, results = improved_main()
    elif choice == '4':
        print("é€€å‡ºç¨‹åº")
    else:
        print("æ— æ•ˆé€‰æ‹©")



'''
if __name__ == "__main__":
    model, graph, results = main()

'''

# è¿è¡Œå›¾æ„å»º
'''if __name__ == "__main__":
    print("=" * 60)
    print("ç¬¬2æ­¥ï¼šæ„å»ºFB15KETå¼‚æ„å›¾")
    print("=" * 60)

    graph_builder = FB15KETGraphBuilder()
    g, entity_map, relation_map = graph_builder.build_heterogeneous_graph(use_relation_types=True)
'''